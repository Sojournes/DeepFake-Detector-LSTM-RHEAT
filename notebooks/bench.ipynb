{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "990dedeb",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-21T11:44:33.451312Z",
     "iopub.status.busy": "2025-04-21T11:44:33.450988Z",
     "iopub.status.idle": "2025-04-21T11:44:42.755157Z",
     "shell.execute_reply": "2025-04-21T11:44:42.754304Z"
    },
    "papermill": {
     "duration": 9.31044,
     "end_time": "2025-04-21T11:44:42.757238",
     "exception": false,
     "start_time": "2025-04-21T11:44:33.446798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting timm==0.6.5\r\n",
      "  Downloading timm-0.6.5-py3-none-any.whl.metadata (45 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.10/dist-packages (from timm==0.6.5) (2.5.1+cu121)\r\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm==0.6.5) (0.20.1+cu121)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.6.5) (3.17.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.6.5) (4.12.2)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.6.5) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.6.5) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.6.5) (2024.9.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4->timm==0.6.5) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.4->timm==0.6.5) (1.3.0)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.6.5) (1.26.4)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.6.5) (11.0.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4->timm==0.6.5) (3.0.2)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->timm==0.6.5) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->timm==0.6.5) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->timm==0.6.5) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->timm==0.6.5) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->timm==0.6.5) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->timm==0.6.5) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision->timm==0.6.5) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision->timm==0.6.5) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->timm==0.6.5) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision->timm==0.6.5) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision->timm==0.6.5) (2024.2.0)\r\n",
      "Downloading timm-0.6.5-py3-none-any.whl (512 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.8/512.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: timm\r\n",
      "  Attempting uninstall: timm\r\n",
      "    Found existing installation: timm 1.0.12\r\n",
      "    Uninstalling timm-1.0.12:\r\n",
      "      Successfully uninstalled timm-1.0.12\r\n",
      "Successfully installed timm-0.6.5\r\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install timm==0.6.5\n",
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a672af42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T11:44:42.765706Z",
     "iopub.status.busy": "2025-04-21T11:44:42.765453Z",
     "iopub.status.idle": "2025-04-21T11:44:54.140819Z",
     "shell.execute_reply": "2025-04-21T11:44:54.140121Z"
    },
    "papermill": {
     "duration": 11.38155,
     "end_time": "2025-04-21T11:44:54.142464",
     "exception": false,
     "start_time": "2025-04-21T11:44:42.760914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import partial\n",
    "\n",
    "import torch # Added\n",
    "import torch.nn as nn # Added\n",
    "import torchvision.models as models # Added\n",
    "\n",
    "from einops import rearrange, reduce, repeat\n",
    "# from mmcv.ops import DeformConv2d\n",
    "from torchvision.ops import DeformConv2d\n",
    "import torch.nn.functional as F\n",
    "from timm.models.layers.activations import *\n",
    "from timm.models.layers import DropPath, trunc_normal_\n",
    "\n",
    "init_alpha_value = 1e-3\n",
    "init_scale_values = 1e-4\n",
    "\n",
    "\n",
    "# ========== For Common ==========\n",
    "class LayerNormConv(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, normalized_shape, eps=1e-6, elementwise_affine=True):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.norm = nn.LayerNorm(normalized_shape, eps, elementwise_affine)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = rearrange(x, 'b c h w -> b h w c').contiguous()\n",
    "\t\tx = self.norm(x)\n",
    "\t\tx = rearrange(x, 'b h w c -> b c h w').contiguous()\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "def get_norm(norm_layer='in_1d'):\n",
    "\teps = 1e-6\n",
    "\tnorm_dict = {\n",
    "\t\t'none': nn.Identity,\n",
    "\t\t'in_1d': partial(nn.InstanceNorm1d, eps=eps),\n",
    "\t\t'in_2d': partial(nn.InstanceNorm2d, eps=eps),\n",
    "\t\t'in_3d': partial(nn.InstanceNorm3d, eps=eps),\n",
    "\t\t'bn_1d': partial(nn.BatchNorm1d, eps=eps),\n",
    "\t\t'bn_2d': partial(nn.BatchNorm2d, eps=eps),\n",
    "\t\t# 'bn_2d': partial(nn.SyncBatchNorm, eps=eps),\n",
    "\t\t'bn_3d': partial(nn.BatchNorm3d, eps=eps),\n",
    "\t\t'gn': partial(nn.GroupNorm, eps=eps),\n",
    "\t\t'ln': partial(nn.LayerNorm, eps=eps),\n",
    "\t\t'lnc': partial(LayerNormConv, eps=eps),\n",
    "\t}\n",
    "\treturn norm_dict[norm_layer]\n",
    "\n",
    "\n",
    "def get_act(act_layer='relu'):\n",
    "\tact_dict = {\n",
    "\t\t'none': nn.Identity,\n",
    "\t\t'sigmoid': Sigmoid,\n",
    "\t\t'swish': Swish,\n",
    "\t\t'mish': Mish,\n",
    "\t\t'hsigmoid': HardSigmoid,\n",
    "\t\t'hswish': HardSwish,\n",
    "\t\t'hmish': HardMish,\n",
    "\t\t'tanh': Tanh,\n",
    "\t\t'relu': nn.ReLU,\n",
    "\t\t'relu6': nn.ReLU6,\n",
    "\t\t'prelu': PReLU,\n",
    "\t\t'gelu': GELU,\n",
    "\t\t'silu': nn.SiLU\n",
    "\t}\n",
    "\treturn act_dict[act_layer]\n",
    "\n",
    "\n",
    "# ========== Individual ==========\n",
    "class MLP(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, in_dim, hid_dim=None, out_dim=None, act_layer='gelu', drop=0.):\n",
    "\t\tsuper().__init__()\n",
    "\t\tout_dim = out_dim or in_dim\n",
    "\t\thid_dim = hid_dim or in_dim\n",
    "\t\tself.fc1 = nn.Conv2d(in_dim, hid_dim, kernel_size=1, stride=1, padding=0)\n",
    "\t\tself.act = get_act(act_layer)()\n",
    "\t\tself.fc2 = nn.Conv2d(hid_dim, out_dim, kernel_size=1, stride=1, padding=0)\n",
    "\t\tself.drop = nn.Dropout(drop)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.fc1(x)\n",
    "\t\tx = self.act(x)\n",
    "\t\tx = self.drop(x)\n",
    "\t\tx = self.fc2(x)\n",
    "\t\tx = self.drop(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class FFN(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, dim, mlp_ratio=4., drop=0., drop_path=0., act_layer='gelu', norm_layer='lnc'):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.norm = get_norm(norm_layer)(dim)\n",
    "\t\tself.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\t\thid_dim = int(dim * mlp_ratio)\n",
    "\t\tself.mlp = MLP(in_dim=dim, hid_dim=hid_dim, out_dim=dim, act_layer=act_layer, drop=drop)\n",
    "\t\tself.gamma_mlp = nn.Parameter(init_scale_values * torch.ones((dim)), requires_grad=True)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tshortcut = x\n",
    "\t\tx = self.norm(x)\n",
    "\t\tx = shortcut + self.drop_path(self.gamma_mlp.unsqueeze(0).unsqueeze(2).unsqueeze(3) * self.mlp(x))\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "# ========== Global and Local Populations ==========\n",
    "class MSA(nn.Module):\n",
    "\tdef __init__(self, dim, dim_head, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.dim_head = dim_head\n",
    "\t\tself.num_head = dim // dim_head\n",
    "\t\tself.scale = self.dim_head ** -0.5\n",
    "\t\t\n",
    "\t\tself.qkv = nn.Conv2d(dim, dim * 3, kernel_size=1, stride=1, padding=0, bias=qkv_bias)\n",
    "\t\tself.attn_drop = nn.Dropout(attn_drop)\n",
    "\t\tself.proj = nn.Conv2d(dim, dim, kernel_size=1, stride=1, padding=0)\n",
    "\t\tself.proj_drop = nn.Dropout(proj_drop)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tB, C, H, W = x.shape\n",
    "\t\t\n",
    "\t\tqkv = self.qkv(x)\n",
    "\t\tqkv = rearrange(qkv, 'b (qkv heads dim_head) h w -> qkv b heads (h w) dim_head', qkv=3, heads=self.num_head, dim_head=self.dim_head).contiguous()\n",
    "\t\tq, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\t\t\n",
    "\t\tattn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "\t\tattn = attn.softmax(dim=-1)\n",
    "\t\tattn = self.attn_drop(attn)\n",
    "\t\t\n",
    "\t\tx = attn @ v\n",
    "\t\tx = rearrange(x, 'b heads (h w) dim_head -> b (heads dim_head) h w', heads=self.num_head, dim_head=self.dim_head, h=H, w=W).contiguous()\n",
    "\t\tx = self.proj(x)\n",
    "\t\tx = self.proj_drop(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class MSA_OP(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, dim, dim_head, window_size, qkv_bias=False, attn_drop=0., proj_drop=0., init_scale_values=1e-4):\n",
    "\t\tsuper().__init__()\n",
    "\t\tassert dim % dim_head == 0\n",
    "\t\tself.window_size = window_size\n",
    "\t\tself.msa = MSA(dim, dim_head, qkv_bias, attn_drop, proj_drop)\n",
    "\t\tself.gamma_msa = nn.Parameter(init_scale_values * torch.ones((dim)), requires_grad=True)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tB, C, H, W = x.shape\n",
    "\t\tif self.window_size <= 0:\n",
    "\t\t\twindow_size_W, window_size_H = W, H\n",
    "\t\telse:\n",
    "\t\t\twindow_size_W, window_size_H = self.window_size, self.window_size\n",
    "\t\tpad_l, pad_t = 0, 0\n",
    "\t\tpad_r = (window_size_W - W % window_size_W) % window_size_W\n",
    "\t\tpad_b = (window_size_H - H % window_size_H) % window_size_H\n",
    "\t\tx = F.pad(x, (pad_l, pad_r, pad_t, pad_b, 0, 0,))\n",
    "\t\t\n",
    "\t\tn1, n2 = (H + pad_b) // window_size_H, (W + pad_r) // window_size_W\n",
    "\t\tx = rearrange(x, 'b c (h1 n1) (w1 n2) -> (b n1 n2) c h1 w1', n1=n1, n2=n2).contiguous()\n",
    "\t\tx = self.gamma_msa.unsqueeze(0).unsqueeze(2).unsqueeze(3) * self.msa(x)\n",
    "\t\tx = rearrange(x, '(b n1 n2) c h1 w1 -> b c (h1 n1) (w1 n2)', n1=n1, n2=n2).contiguous()\n",
    "\t\t\n",
    "\t\tif pad_r > 0 or pad_b > 0:\n",
    "\t\t\tx = x[:, :, :H, :W].contiguous()\n",
    "\t\t\t\n",
    "\t\treturn x\n",
    "\t\n",
    "\t\n",
    "class DMSA(nn.Module):\n",
    "\tdef __init__(self, dim, dim_head, kernel_size, stride, qkv_bias=False, attn_drop=0., proj_drop=0., d_groups=3):\n",
    "\t\tsuper().__init__()\n",
    "\t\tassert dim % dim_head == 0\n",
    "\t\tself.kernel_size = kernel_size\n",
    "\t\tself.stride = stride\n",
    "\t\tself.dim = dim\n",
    "\t\tself.dim_head = dim_head\n",
    "\t\tself.num_head = dim // dim_head\n",
    "\t\tself.scale = self.dim_head ** -0.5\n",
    "\t\tself.d_groups = d_groups\n",
    "\t\tself.n_group_dim = self.dim // self.d_groups\n",
    "\t\tself.offset_range_factor = 2\n",
    "\t\t\n",
    "\t\tself.conv_offset_modulation = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(self.n_group_dim, self.n_group_dim, self.kernel_size, self.stride, self.kernel_size // 2, groups=self.n_group_dim),\n",
    "\t\t\tget_norm('bn_2d')(self.n_group_dim),\n",
    "\t\t\tnn.GELU(),\n",
    "\t\t\tnn.Conv2d(self.n_group_dim, 3, 1, 1, 0, bias=False)\n",
    "\t\t)\n",
    "\t\tself.modulation_act = get_act('sigmoid')()\n",
    "\t\tself.q = nn.Conv2d(dim, dim * 1, kernel_size=1, stride=1, padding=0, bias=qkv_bias)\n",
    "\t\tself.kv = nn.Conv2d(dim, dim * 2, kernel_size=1, stride=1, padding=0, bias=qkv_bias)\n",
    "\t\tself.proj = nn.Conv2d(dim, dim, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "\t\tself.attn_drop = nn.Dropout(attn_drop)\n",
    "\t\tself.proj_drop = nn.Dropout(proj_drop)\n",
    "\t\n",
    "\t@torch.no_grad()\n",
    "\tdef _get_ref_points(self, H, W, B, dtype, device):\n",
    "\t\tref_y, ref_x = torch.meshgrid(torch.linspace(0.5, H - 0.5, H, dtype=dtype, device=device),\n",
    "\t\t\t\t\t\t\t\t\t  torch.linspace(0.5, W - 0.5, W, dtype=dtype, device=device))\n",
    "\t\tref = torch.stack((ref_y, ref_x), -1)\n",
    "\t\tref[..., 1].div_(W).mul_(2).sub_(1)\n",
    "\t\tref[..., 0].div_(H).mul_(2).sub_(1)\n",
    "\t\tref = ref[None, ...].expand(B * self.d_groups, -1, -1, -1)  # B * g H W 2\n",
    "\t\treturn ref\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tB, C, H, W = x.shape\n",
    "\t\tq = self.q(x)\n",
    "\t\tq_off = rearrange(q, 'b (g c) h w -> (b g) c h w', g=self.d_groups, c=self.n_group_dim).contiguous()\n",
    "\t\toffset_modulation = self.conv_offset_modulation(q_off)  # bg 3 h w\n",
    "\t\toffset, modulation = offset_modulation[:, 0:2, :, :], self.modulation_act(offset_modulation[:, 2:3, :, :])  # bg 2 h w, bg 1 h w\n",
    "\t\tH_off, W_off = offset.size(2), offset.size(3)\n",
    "\t\t\n",
    "\t\toffset_range = torch.tensor([1.0 / H_off, 1.0 / W_off], device=x.device).reshape(1, 2, 1, 1)\n",
    "\t\toffset = offset.tanh().mul(offset_range).mul(self.offset_range_factor)\n",
    "\t\toffset = rearrange(offset, 'b c h w -> b h w c').contiguous()\n",
    "\t\treference = self._get_ref_points(H_off, W_off, B, x.dtype, x.device)\n",
    "\t\tpos = offset + reference\n",
    "\t\t\n",
    "\t\tx_sampled = F.grid_sample(input=x.reshape(B * self.d_groups, self.n_group_dim, H, W),\n",
    "\t\t\t\t\t\t\t\t  grid=pos[..., (1, 0)],  # y, x -> x, y\n",
    "\t\t\t\t\t\t\t\t  mode='bilinear', align_corners=True)  # B * g, Cg, Hg, Wg\n",
    "\t\tx_sampled *= modulation.sigmoid()\n",
    "\t\tx_sampled = rearrange(x_sampled, '(b g) c h w -> b (g c) h w', b=B, g=self.d_groups).contiguous()\n",
    "\t\tq = rearrange(q, 'b (heads dim_head) h w -> b heads (h w) dim_head', heads=self.num_head,\n",
    "\t\t\t\t\t  dim_head=self.dim_head).contiguous()\n",
    "\t\tkv = self.kv(x_sampled)\n",
    "\t\tkv = rearrange(kv, 'b (kv heads dim_head) h w -> kv b heads (h w) dim_head', kv=2, heads=self.num_head,\n",
    "\t\t\t\t\t   dim_head=self.dim_head).contiguous()\n",
    "\t\tk, v = kv[0], kv[1]\n",
    "\t\t\n",
    "\t\tattn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "\t\tattn = attn.softmax(dim=-1)\n",
    "\t\tattn = self.attn_drop(attn)\n",
    "\t\t\n",
    "\t\tx = attn @ v\n",
    "\t\tx = rearrange(x, 'b heads (h w) dim_head -> b (heads dim_head) h w', heads=self.num_head,\n",
    "\t\t\t\t\t  dim_head=self.dim_head, h=H, w=W).contiguous()\n",
    "\t\tx = self.proj(x)\n",
    "\t\tx = self.proj_drop(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class DMSA_OP(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, dim, dim_head, window_size, kernel_size, stride, qkv_bias=False, attn_drop=0., proj_drop=0., d_groups=3):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.window_size = window_size\n",
    "\t\tself.mdmsa = DMSA(dim, dim_head, kernel_size, stride, qkv_bias, attn_drop, proj_drop, d_groups)\n",
    "\t\tself.gamma_mdmsa = nn.Parameter(init_scale_values * torch.ones((dim)), requires_grad=True)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tB, C, H, W = x.shape\n",
    "\t\tif self.window_size <= 0:\n",
    "\t\t\twindow_size_W, window_size_H = W, H\n",
    "\t\telse:\n",
    "\t\t\twindow_size_W, window_size_H = self.window_size, self.window_size\n",
    "\t\tpad_l, pad_t = 0, 0\n",
    "\t\tpad_r = (window_size_W - W % window_size_W) % window_size_W\n",
    "\t\tpad_b = (window_size_H - H % window_size_H) % window_size_H\n",
    "\t\tx = F.pad(x, (pad_l, pad_r, pad_t, pad_b, 0, 0,))\n",
    "\t\t\n",
    "\t\tn1, n2 = (H + pad_b) // window_size_H, (W + pad_r) // window_size_W\n",
    "\t\tx = rearrange(x, 'b c (h1 n1) (w1 n2) -> (b n1 n2) c h1 w1', n1=n1, n2=n2).contiguous()\n",
    "\t\tx = self.gamma_mdmsa.unsqueeze(0).unsqueeze(2).unsqueeze(3) * self.mdmsa(x)\n",
    "\t\tx = rearrange(x, '(b n1 n2) c h1 w1 -> b c (h1 n1) (w1 n2)', n1=n1, n2=n2).contiguous()\n",
    "\t\t\n",
    "\t\tif pad_r > 0 or pad_b > 0:\n",
    "\t\t\tx = x[:, :, :H, :W].contiguous()\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class Conv_OP(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, dim, kernel_size, stride=1):\n",
    "\t\tsuper().__init__()\n",
    "\t\tpadding = math.ceil((kernel_size - stride) / 2)\n",
    "\t\tself.conv1 = nn.Conv2d(dim, dim, kernel_size, stride, padding, groups=dim)\n",
    "\t\tself.norm1 = get_norm('bn_2d')(dim)\n",
    "\t\tself.act1 = get_act('silu')()\n",
    "\t\tself.conv2 = nn.Conv2d(dim, dim, 1, 1, 0)\n",
    "\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.conv1(x)\n",
    "\t\tx = self.norm1(x)\n",
    "\t\tx = self.act1(x)\n",
    "\t\tx = self.conv2(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class DCN2_OP(nn.Module):\n",
    "\t# ref: https://github.com/WenmuZhou/DBNet.pytorch/blob/678b2ae55e018c6c16d5ac182558517a154a91ed/models/backbone/resnet.py\n",
    "\tdef __init__(self, dim, kernel_size=3, stride=1, deform_groups=4):\n",
    "\t\tsuper().__init__()\n",
    "\t\toffset_channels = kernel_size * kernel_size * 2\n",
    "\t\tself.conv1_offset = nn.Conv2d(dim, deform_groups * offset_channels, kernel_size=3, stride=stride, padding=1)\n",
    "\t\tself.conv1 = DeformConv2d(dim, dim, kernel_size=kernel_size, padding=kernel_size // 2, bias=False)\n",
    "\t\tself.norm1 = get_norm('bn_2d')(dim)\n",
    "\t\tself.act1 = get_act('silu')()\n",
    "\t\tself.conv2 = nn.Conv2d(dim, dim, 1, 1, 0)\n",
    "\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\toffset = self.conv1_offset(x)\n",
    "\t\tx = self.conv1(x, offset)\n",
    "\t\tx = self.norm1(x)\n",
    "\t\tx = self.act1(x)\n",
    "\t\tx = self.conv2(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class GLI(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, in_dim, dim_head, window_size, kernel_size=5, qkv_bias=False, drop=0., attn_drop=0.,\n",
    "\t\t\t\t drop_path=0., act_layer='gelu', norm_layer='bn_2d',\n",
    "\t\t\t\t op_names=['msa', 'mdmsa', 'conv', 'dcn'], d_group=3, gli_split=False, gli_weight=True, gli_ratio=None):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.op_names = op_names\n",
    "\t\tself.gli_split = gli_split\n",
    "\t\tself.gli_weight = gli_weight\n",
    "\t\tself.gli_ratio = gli_ratio\n",
    "\t\tself.op_num = len(op_names)\n",
    "\t\tself.norm = get_norm(norm_layer)(in_dim)\n",
    "\t\tself.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\t\tif self.op_num == 1:\n",
    "\t\t\tdims = [in_dim]\n",
    "\t\telse:\n",
    "\t\t\tif gli_split:\n",
    "\t\t\t\tif gli_ratio:\n",
    "\t\t\t\t\tassert self.op_num == 2\n",
    "\t\t\t\t\tdims = [int(in_dim * gli_ratio), round(in_dim * (1 - gli_ratio))]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tdim = in_dim // self.op_num\n",
    "\t\t\t\t\tassert dim * self.op_num == in_dim\n",
    "\t\t\t\t\tdims = [dim] * self.op_num\n",
    "\t\t\telse:\n",
    "\t\t\t\tdims = [in_dim] * self.op_num\n",
    "\t\tself.dims = dims\n",
    "\t\tself.ops = nn.ModuleList()\n",
    "\t\tfor idx, op_name in enumerate(op_names):\n",
    "\t\t\tif op_name in ['conv', 'c']:\n",
    "\t\t\t\top = Conv_OP(dims[idx], kernel_size, stride=1)\n",
    "\t\t\telif op_name in ['dcn', 'dc']:\n",
    "\t\t\t\top = DCN2_OP(dims[idx], kernel_size, stride=1, deform_groups=d_group)\n",
    "\t\t\telif op_name in ['msa', 'm']:\n",
    "\t\t\t\top = MSA_OP(dims[idx], dim_head, window_size, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n",
    "\t\t\telif op_name in ['mdmsa', 'dm']:\n",
    "\t\t\t\top = DMSA_OP(dims[idx], dim_head, window_size, kernel_size=5, stride=1, qkv_bias=True, attn_drop=attn_drop, proj_drop=drop, d_groups=d_group)\n",
    "\t\t\telse:\n",
    "\t\t\t\traise 'invalid \\'{}\\' operation'.format(op_name)\n",
    "\t\t\tself.ops.append(op)\n",
    "\t\tif self.op_num > 1 and gli_weight:\n",
    "\t\t\tself.alphas = nn.Parameter(init_alpha_value * torch.ones(self.op_num), requires_grad=True)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tshortcut = x\n",
    "\t\tx = self.norm(x)\n",
    "\t\tif self.op_num == 1:\n",
    "\t\t\tx = self.ops[0](x)\n",
    "\t\telse:\n",
    "\t\t\tif self.gli_split:\n",
    "\t\t\t\tif self.gli_ratio:\n",
    "\t\t\t\t\txs = [x[:, :self.dims[0], :, :], x[:, self.dims[0]:, :, :]]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\txs = torch.chunk(x, self.op_num, dim=1)\n",
    "\t\t\telse:\n",
    "\t\t\t\txs = [x] * self.op_num\n",
    "\t\t\tif self.gli_weight:\n",
    "\t\t\t\talphas = F.softmax(self.alphas, dim=-1)\n",
    "\t\t\t\tif self.gli_split:\n",
    "\t\t\t\t\tif self.gli_ratio:\n",
    "\t\t\t\t\t\tx = torch.cat([self.ops[i](xs[i]) * alphas[i] for i in range(self.op_num)], dim=1).contiguous()\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\txs = torch.cat([self.ops[i](xs[i]).unsqueeze(dim=-1) * alphas[i] for i in range(self.op_num)], dim=-1)\n",
    "\t\t\t\t\t\tx = rearrange(xs, 'b c h w n -> b (c n) h w').contiguous()\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\txs = torch.cat([self.ops[i](xs[i]).unsqueeze(dim=-1) * alphas[i] for i in range(self.op_num)], dim=-1)\n",
    "\t\t\t\t\tx = reduce(xs, 'b c h w n -> b c h w', 'mean').contiguous()\n",
    "\t\t\telse:\n",
    "\t\t\t\tif self.gli_split:\n",
    "\t\t\t\t\tx = torch.cat([self.ops[i](xs[i]) for i in range(self.op_num)], dim=1)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\txs = torch.cat([self.ops[i](xs[i]).unsqueeze(dim=-1) for i in range(self.op_num)], dim=-1)\n",
    "\t\t\t\t\tx = reduce(xs, 'b c h w n -> b c h w', 'mean').contiguous()\n",
    "\t\tx = shortcut + self.drop_path(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "# ========== Multi-Scale Populations ==========\n",
    "class MSP(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, in_dim, emb_dim, kernel_size=3, c_group=-1, stride=1, dilations=[1, 2, 3], msra_mode='cat',\n",
    "\t\t\t\t act_layer='silu', norm_layer='bn_2d', msra_weight=True):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.msra_mode = msra_mode\n",
    "\t\tself.msra_weight = msra_weight\n",
    "\t\tself.dilation_num = len(dilations)\n",
    "\t\tassert in_dim % c_group == 0\n",
    "\t\tc_group = (in_dim if c_group == -1 else c_group) if stride == 1 else 1\n",
    "\t\tself.convs = nn.ModuleList()\n",
    "\t\tfor i in range(len(dilations)):\n",
    "\t\t\tpadding = math.ceil(((kernel_size - 1) * dilations[i] + 1 - stride) / 2)\n",
    "\t\t\tself.convs.append(nn.Sequential(\n",
    "\t\t\t\tnn.Conv2d(in_dim, emb_dim, kernel_size, stride, padding, dilations[i], groups=c_group),\n",
    "\t\t\t\tget_act(act_layer)(emb_dim)))\n",
    "\t\tif self.dilation_num > 1 and msra_weight:\n",
    "\t\t\tself.alphas = nn.Parameter(init_alpha_value * torch.ones(self.dilation_num), requires_grad=True)\n",
    "\t\tself.conv_out = nn.Conv2d(emb_dim * (self.dilation_num if msra_mode == 'cat' else 1), emb_dim, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\t# B, C, H, W\n",
    "\t\tif self.dilation_num == 1:\n",
    "\t\t\tx = self.convs[0](x)\n",
    "\t\telse:\n",
    "\t\t\tif self.msra_weight:\n",
    "\t\t\t\talphas = F.softmax(self.alphas, dim=-1)\n",
    "\t\t\t\tx = torch.cat([self.convs[i](x).unsqueeze(dim=-1) * alphas[i] for i in range(self.dilation_num)], dim=-1)\n",
    "\t\t\telse:\n",
    "\t\t\t\tx = torch.cat([self.convs[i](x).unsqueeze(dim=-1) for i in range(self.dilation_num)], dim=-1)\n",
    "\t\t\tif self.msra_mode == 'cat':\n",
    "\t\t\t\tx = rearrange(x, 'b c h w n -> b (c n) h w').contiguous()\n",
    "\t\t\telif self.msra_mode == 'sum':\n",
    "\t\t\t\tx = reduce(x, 'b c h w n -> b c h w', 'mean').contiguous()\n",
    "\t\tx = self.conv_out(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class MSRA(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, in_dim, emb_dim, kernel_size=3, c_group=-1, stride=1, dilations=[1, 2, 3], msra_mode='cat',\n",
    "\t\t\t\t act_layer='silu', norm_layer='bn_2d', msra_weight=True, msra_norm=True, msra_skip=True, drop_path=0.):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.norm = get_norm(norm_layer)(in_dim) if msra_norm else nn.Identity()\n",
    "\t\tself.msp = MSP(in_dim, emb_dim, kernel_size, c_group, stride, dilations, msra_mode, act_layer, norm_layer, msra_weight)\n",
    "\t\tself.msra_skip = msra_skip\n",
    "\t\tif msra_skip:\n",
    "\t\t\tif stride == 1:\n",
    "\t\t\t\tself.skip_conv = nn.Identity()\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.skip_conv = nn.Sequential(nn.AvgPool2d(kernel_size=stride, stride=stride, ceil_mode=True, count_include_pad=False),\n",
    "\t\t\t\t\t\t\t\t\t\t\t   nn.Conv2d(in_dim, emb_dim, 1, stride=1, padding=0, bias=False),\n",
    "\t\t\t\t\t\t\t\t\t\t\t   get_norm(norm_layer)(emb_dim))\n",
    "\t\t\tself.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tshortcut = x\n",
    "\t\tx = self.msp(self.norm(x))\n",
    "\t\tif self.msra_skip:\n",
    "\t\t\tx = self.skip_conv(shortcut) + self.drop_path(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "# ========== Block ==========\n",
    "class EATBlock(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, in_dim, emb_dim, kernel_size=3, stride=1, dilations=[1, 2, 3], norms=['bn_2d', 'bn_2d', 'bn_2d'],\n",
    "\t\t\t\t msra_mode='cat', msra_weight=True, msra_norm=True, msra_skip=True,\n",
    "\t\t\t\t dim_head=6, window_size=7, qkv_bias=True, drop=0., attn_drop=0., drop_path=0.,\n",
    "\t\t\t\t op_names=['msa', 'conv'], d_group=3, c_group=-1, gli_split=False, gli_weight=True, gli_ratio=None, mlp_ratio=4., ):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.layer1 = MSRA(in_dim, emb_dim, kernel_size, c_group, stride, dilations, msra_mode, 'silu', norms[0],\n",
    "\t\t\t\t\t\t\t\t\tmsra_weight, msra_norm, msra_skip, drop_path)\n",
    "\t\tself.layer2 = GLI(emb_dim, dim_head, window_size, 5, qkv_bias,\n",
    "\t\t\t\t\t\t\t\t   drop, attn_drop, drop_path, 'silu', norms[1],\n",
    "\t\t\t\t\t\t\t\t   op_names, d_group, gli_split, gli_weight, gli_ratio)\n",
    "\t\tself.layer3 = FFN(emb_dim, mlp_ratio, drop, drop_path, 'gelu', norms[2])\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.layer1(x)\n",
    "\t\tx = self.layer2(x)\n",
    "\t\tx = self.layer3(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "# ========== Task-related Head ==========\n",
    "class MCA(nn.Module):\n",
    "\tdef __init__(self, dim, dim_head=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.dim_head = dim_head\n",
    "\t\tself.num_head = dim // dim_head\n",
    "\t\tself.scale = self.dim_head ** -0.5\n",
    "\t\t\n",
    "\t\tself.q = nn.Conv2d(dim, dim * 1, kernel_size=1, stride=1, padding=0, bias=qkv_bias)\n",
    "\t\tself.kv = nn.Conv2d(dim, dim * 2, kernel_size=1, stride=1, padding=0, bias=qkv_bias)\n",
    "\t\tself.attn_drop = nn.Dropout(attn_drop)\n",
    "\t\tself.proj = nn.Conv2d(dim, dim, kernel_size=1, stride=1, padding=0, )\n",
    "\t\tself.proj_drop = nn.Dropout(proj_drop)\n",
    "\t\n",
    "\tdef forward(self, x, xq):\n",
    "\t\tB, C, H, W = x.shape\n",
    "\t\t_, _, Hq, Wq = xq.shape\n",
    "\t\t\n",
    "\t\tq = self.q(xq)\n",
    "\t\tkv = self.kv(x)\n",
    "\t\tq = rearrange(q, 'b (q heads dim_head) h w -> q b heads (h w) dim_head', q=1, heads=self.num_head,\n",
    "\t\t\t\t\t  dim_head=self.dim_head).contiguous()\n",
    "\t\tkv = rearrange(kv, 'b (kv heads dim_head) h w -> kv b heads (h w) dim_head', kv=2, heads=self.num_head,\n",
    "\t\t\t\t\t   dim_head=self.dim_head).contiguous()\n",
    "\t\tq, k, v = q[0], kv[0], kv[1]\n",
    "\t\t\n",
    "\t\tattn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "\t\tattn = attn.softmax(dim=-1)\n",
    "\t\tattn = self.attn_drop(attn)\n",
    "\t\t\n",
    "\t\tx = attn @ v\n",
    "\t\tx = rearrange(x, 'b heads (h w) dim_head -> b (heads dim_head) h w', heads=self.num_head,\n",
    "\t\t\t\t\t  dim_head=self.dim_head, h=Hq, w=Wq).contiguous()\n",
    "\t\tx = self.proj(x)\n",
    "\t\tx = self.proj_drop(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class TRHead(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, dim, dim_head, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.,\n",
    "\t\t\t\t drop_path=0., act_layer='gelu', norm_layer='lnc'):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.norm_kv = get_norm(norm_layer)(dim)\n",
    "\t\tself.norm1 = get_norm(norm_layer)(dim)\n",
    "\t\tself.attn = MCA(dim, dim_head=dim_head, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n",
    "\t\tself.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\t\tself.norm2 = get_norm(norm_layer)(dim)\n",
    "\t\thid_dim = int(dim * mlp_ratio)\n",
    "\t\tself.mlp = MLP(in_dim=dim, hid_dim=hid_dim, out_dim=dim, act_layer=act_layer, drop=drop)\n",
    "\t\n",
    "\tdef forward(self, x, xq):\n",
    "\t\txq = xq + self.drop_path(self.attn(self.norm_kv(x), self.norm1(xq)))\n",
    "\t\txq = xq + self.drop_path(self.mlp(self.norm2(xq)))\n",
    "\t\treturn xq\n",
    "\n",
    "\n",
    "class HaarWaveletTransform(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(HaarWaveletTransform, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # Projection layer to match the number of channels if necessary\n",
    "        if in_channels != out_channels:\n",
    "            self.proj = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        else:\n",
    "            self.proj = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, channels, height, width)\n",
    "        batch_size, channels, height, width = x.size()\n",
    "\n",
    "        # Ensure input dimensions are even\n",
    "        if height % 2 != 0 or width % 2 != 0:\n",
    "            pad_h = 1 if height % 2 != 0 else 0\n",
    "            pad_w = 1 if width % 2 != 0 else 0\n",
    "            x = nn.functional.pad(x, (0, pad_w, 0, pad_h))\n",
    "            height += pad_h\n",
    "            width += pad_w\n",
    "\n",
    "        # Apply Haar wavelet transform on each channel\n",
    "        transformed = []\n",
    "        for c in range(channels):\n",
    "            # Convert tensor to numpy\n",
    "            img = x[:, c, :, :].cpu().detach().numpy()\n",
    "\n",
    "            # Apply DWT2\n",
    "            coeffs = pywt.dwt2(img, 'haar')\n",
    "            LL, (LH, HL, HH) = coeffs\n",
    "\n",
    "            # Compute the Haar feature (XRC_cap)\n",
    "            XRC_cap = np.sqrt(LH**2 + HL**2 + HH**2)\n",
    "\n",
    "            # Convert back to tensor\n",
    "            XRC_cap_tensor = torch.tensor(XRC_cap, dtype=x.dtype).to(x.device)\n",
    "            transformed.append(XRC_cap_tensor.unsqueeze(1))  # (batch, 1, H/2, W/2)\n",
    "\n",
    "        # Stack all channels\n",
    "        transformed = torch.cat(transformed, dim=1)  # (batch, channels, H/2, W/2)\n",
    "\n",
    "        # Upsample to match original spatial dimensions\n",
    "        transformed = nn.functional.interpolate(transformed, size=(height, width), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Project to match output channels\n",
    "        transformed = self.proj(transformed)\n",
    "\n",
    "        return transformed\n",
    "\n",
    "class EATFormer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim=3, num_classes=1000,\n",
    "                 depths=[2, 2, 6, 2], embed_dims=[64, 128, 256, 512], dim_heads=[32, 32, 32, 32],\n",
    "                 window_sizes=[7, 7, 7, 7], kernel_sizes=[3, 3, 3, 3], down_mode='kernel',\n",
    "                 dilations=[[1], [1], [1, 2, 3], [1, 2]], norms=['bn_2d', 'bn_2d', 'bn_2d'],\n",
    "                 msra_mode='sum', msra_weight=True, msra_norm=True, msra_skip=True,\n",
    "                 qkv_bias=True, drop=0., attn_drop=0., drop_path=0.,\n",
    "                 op_names=[['conv'], ['conv'], ['msa', 'conv'], ['msa', 'conv']],\n",
    "                 d_groups=[3, 3, 3, 3], c_groups=[-1, -1, -1, -1], gli_split=False, gli_weight=True, gli_ratio=None,\n",
    "                 mlp_ratio=4., cls_head_nums=0):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.cls_head_nums = cls_head_nums\n",
    "        self.in_dim = in_dim # Store in_dim\n",
    "        dprs = [x.item() for x in torch.linspace(0, drop_path, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "        # === DenseNet Integration START ===\n",
    "        densenet = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT) # Load pretrained DenseNet121\n",
    "        self.densenet_features = densenet.features # Keep only the feature extractor part\n",
    "        # Freeze DenseNet parameters\n",
    "        for param in self.densenet_features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Adaptation layer: Maps DenseNet output (1024, H/32, W/32) to stage0 input (in_dim, H, W)\n",
    "        # Note: This requires knowing the spatial reduction factor (32) of densenet_features\n",
    "        # Using ConvTranspose2d for upsampling might be one way.\n",
    "        # A simpler approach using Upsample + Conv2d:\n",
    "        self.adaptation_layer = nn.Sequential(\n",
    "            # Example: Upsample by 32x spatially (may need adjustment based on actual DenseNet output size)\n",
    "            nn.Upsample(scale_factor=32, mode='bilinear', align_corners=False),\n",
    "            # Reduce channels from 1024 (DenseNet output) to in_dim (stage0 input)\n",
    "            nn.Conv2d(1024, self.in_dim, kernel_size=1, stride=1, padding=0),\n",
    "            get_norm('bn_2d')(self.in_dim), # Optional normalization\n",
    "            get_act('relu')() # Optional activation\n",
    "        )\n",
    "        # === DenseNet Integration END ===\n",
    "\n",
    "        # Stage 0: Initial convolutional blocks\n",
    "        # Takes input adapted from DenseNet (in_dim, H, W)\n",
    "        self.stage0 = nn.ModuleList([\n",
    "            MSRA(self.in_dim, embed_dims[0] // 2, kernel_size=3, c_group=1, stride=2, dilations=[1], msra_mode='sum',\n",
    "                          act_layer='silu', norm_layer='bn_2d', msra_weight=False,\n",
    "                          msra_norm=False, msra_skip=False),\n",
    "            MSRA(embed_dims[0] // 2, embed_dims[0], kernel_size=3, c_group=1, stride=2, dilations=[1], msra_mode='sum',\n",
    "                          act_layer='silu', norm_layer='bn_2d', msra_weight=False,\n",
    "                          msra_norm=True, msra_skip=False)\n",
    "        ])\n",
    "\n",
    "        # Removed Haar residual parts\n",
    "\n",
    "        # Build stages (stage1, stage2, etc.)\n",
    "        emb_dim_pre = embed_dims[0]\n",
    "        for i in range(len(depths)):\n",
    "            layers = []\n",
    "            dpr = dprs[sum(depths[:i]):sum(depths[:i + 1])] #\n",
    "            for j in range(depths[i]):\n",
    "                stride = 2 if j == 0 and i > 0 else 1\n",
    "                kernel_size = stride if stride > 1 and down_mode == 'patch' else kernel_sizes[i]\n",
    "                layers.append(EATBlock(emb_dim_pre, emb_dim=embed_dims[i], kernel_size=kernel_size, stride=stride,\n",
    "                                      dilations=dilations[i], norms=norms, msra_mode=msra_mode, msra_weight=msra_weight, msra_norm=msra_norm, #\n",
    "                                      msra_skip=msra_skip, dim_head=dim_heads[i], window_size=window_sizes[i],\n",
    "                                      qkv_bias=qkv_bias, drop=drop, attn_drop=attn_drop, drop_path=dpr[j], #\n",
    "                                      op_names=op_names[i], d_group=d_groups[i], c_group=c_groups[i],\n",
    "                                      gli_split=gli_split, gli_weight=gli_weight, gli_ratio=gli_ratio, mlp_ratio=mlp_ratio,)) #\n",
    "                emb_dim_pre = embed_dims[i]\n",
    "            self.__setattr__(f'stage{i + 1}', nn.ModuleList(layers)) #\n",
    "\n",
    "        # Classification head\n",
    "        if cls_head_nums:\n",
    "            self.cls_token = nn.Parameter(torch.zeros(1, embed_dims[-1], 1, 1))\n",
    "            layers = [TRHead(embed_dims[-1], dim_heads[-1], mlp_ratio=mlp_ratio,\n",
    "                             qkv_bias=qkv_bias, drop=drop, attn_drop=attn_drop, drop_path=0.)\n",
    "                      for _ in range(cls_head_nums)] #\n",
    "            self.stage_cls = nn.ModuleList(layers) #\n",
    "        else:\n",
    "            self.cls_token, self.stage_cls = None, nn.ModuleList() #\n",
    "\n",
    "        self.norm = nn.BatchNorm2d(embed_dims[-1]) #\n",
    "        self.head = nn.Linear(embed_dims[-1], num_classes) if num_classes > 0 else nn.Identity() #\n",
    "\n",
    "        if self.cls_token is not None:\n",
    "            trunc_normal_(self.cls_token, std=.02) #\n",
    "        self.apply(self._init_weights) #\n",
    "\n",
    "    def _init_weights(self, m): #\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0) #\n",
    "        elif isinstance(m, nn.LayerNorm): #\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def no_weight_decay(self): #\n",
    "        return {'pos_embed', 'cls_token'}\n",
    "\n",
    "    def no_weight_decay_keywords(self): #\n",
    "        return {'alpha', 'gamma', 'beta'}\n",
    "\n",
    "    def no_ft_keywords(self): #\n",
    "        return {}\n",
    "\n",
    "    def ft_head_keywords(self): #\n",
    "        return {'head.weight', 'head.bias'}, self.num_classes\n",
    "\n",
    "    def get_classifier(self): #\n",
    "        return self.head\n",
    "\n",
    "    def reset_classifier(self, num_classes, global_pool=''): #\n",
    "        self.num_classes = num_classes\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def check_bn(self): #\n",
    "        for name, m in self.named_modules():\n",
    "            if isinstance(m, torch.nn.modules.batchnorm._NormBase): #\n",
    "                m.running_mean.nan_to_num_(nan=0, posinf=1, neginf=-1)\n",
    "                m.running_var.nan_to_num_(nan=0, posinf=1, neginf=-1)\n",
    "\n",
    "    # Removed adjust_haar_size method\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        # === DenseNet Integration START ===\n",
    "        # 1. Pass input through DenseNet feature extractor\n",
    "        x_dense = self.densenet_features(x) # Output: (B, 1024, H/32, W/32)\n",
    "\n",
    "        # 2. Adapt DenseNet features to match stage0 input requirements\n",
    "        x_adapted = self.adaptation_layer(x_dense) # Output: (B, in_dim, H, W)\n",
    "        # === DenseNet Integration END ===\n",
    "\n",
    "        # 3. Pass adapted features through stage0\n",
    "        # Note: Original input 'x' is replaced by 'x_adapted' here\n",
    "        for blk in self.stage0: #\n",
    "            x_adapted = blk(x_adapted)\n",
    "        x = x_adapted # Use the output of stage0 for subsequent stages\n",
    "\n",
    "        # Removed Haar feature combination\n",
    "\n",
    "        # Pass through remaining stages\n",
    "        for blk in self.stage1:\n",
    "            x = blk(x)\n",
    "        for blk in self.stage2: #\n",
    "            x = blk(x)\n",
    "        for blk in self.stage3: #\n",
    "            x = blk(x)\n",
    "        for blk in self.stage4: #\n",
    "            x = blk(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.forward_features(x) #\n",
    "        if self.cls_head_nums: #\n",
    "            cls_token = self.cls_token.expand(B, -1, -1, -1)\n",
    "            for blk in self.stage_cls:\n",
    "                cls_token = blk(x, cls_token)\n",
    "            x = cls_token\n",
    "\n",
    "        x = self.norm(x) #\n",
    "        x = reduce(x, 'b c h w -> b c', 'mean').contiguous() #\n",
    "        x = self.head(x) #\n",
    "        return x\n",
    "\n",
    "\n",
    "def eatformer_mobile(pretrained=False, **kwargs):\n",
    "\tmodel = EATFormer(\n",
    "\t\t# in_dim=3, num_classes=1000,\n",
    "\t\tdepths=[1, 1, 4, 1], embed_dims=[48, 64, 160, 256], dim_heads=[16, 16, 20, 32], window_sizes=[7, 7, 7, 7],\n",
    "\t\tkernel_sizes=[3, 3, 3, 3], down_mode='patch', dilations=[[1], [1], [1, 2], [1]],\n",
    "\t\tnorms=['bn_2d', 'bn_2d', 'bn_2d'], msra_mode='sum', msra_weight=True, msra_norm=True, msra_skip=True,\n",
    "\t\tqkv_bias=True, drop=0., attn_drop=0., drop_path=0.03,\n",
    "\t\top_names=[['conv'], ['conv'], ['mdmsa', 'conv'], ['mdmsa', 'conv']],\n",
    "\t\td_groups=[2, 2, 2, 2], c_groups=[-1, -1, -1, -1], gli_split=True, gli_weight=True,\n",
    "\t\tmlp_ratio=2.5, cls_head_nums=0, **kwargs)\n",
    "\treturn model\n",
    "\n",
    "\n",
    "def eatformer_lite(pretrained=False, **kwargs):\n",
    "\tmodel = EATFormer(\n",
    "\t\t# in_dim=3, num_classes=1000,\n",
    "\t\tdepths=[1, 2, 6, 1], embed_dims=[64, 128, 192, 256], dim_heads=[32, 32, 32, 32], window_sizes=[7, 7, 7, 7],\n",
    "\t\tkernel_sizes=[3, 3, 3, 3], down_mode='patch', dilations=[[1], [1], [1, 2], [1]],\n",
    "\t\tnorms=['bn_2d', 'bn_2d', 'bn_2d'], msra_mode='sum', msra_weight=True, msra_norm=True, msra_skip=True,\n",
    "\t\tqkv_bias=True, drop=0., attn_drop=0., drop_path=0.03,\n",
    "\t\top_names=[['conv'], ['conv'], ['mdmsa', 'conv'], ['mdmsa', 'conv']],\n",
    "\t\td_groups=[2, 2, 2, 2], c_groups=[-1, -1, -1, -1], gli_split=True, gli_weight=True,\n",
    "\t\tmlp_ratio=3, cls_head_nums=0, **kwargs)\n",
    "\treturn model\n",
    "\n",
    "\n",
    "def eatformer_tiny(pretrained=False, **kwargs):\n",
    "\tmodel = EATFormer(\n",
    "\t\t# in_dim=3, num_classes=1000,\n",
    "\t\tdepths=[2, 2, 6, 2], embed_dims=[64, 128, 192, 256], dim_heads=[32, 32, 32, 32], window_sizes=[7, 7, 7, 7],\n",
    "\t\tkernel_sizes=[3, 3, 3, 3], down_mode='kernel', dilations=[[1], [1], [1, 2, 3], [1, 2]],\n",
    "\t\tnorms=['bn_2d', 'bn_2d', 'bn_2d'], msra_mode='sum', msra_weight=True, msra_norm=True, msra_skip=True,\n",
    "\t\tqkv_bias=True, drop=0., attn_drop=0., drop_path=0.05,\n",
    "\t\top_names=[['conv'], ['conv'], ['mdmsa', 'conv'], ['mdmsa', 'conv']],\n",
    "\t\td_groups=[2, 2, 2, 2], c_groups=[-1, -1, -1, -1], gli_split=True, gli_weight=True,\n",
    "\t\tmlp_ratio=4., cls_head_nums=0, **kwargs)\n",
    "\treturn model\n",
    "\n",
    "\n",
    "def eatformer_mini(pretrained=False, **kwargs):\n",
    "\tmodel = EATFormer(\n",
    "\t\t# in_dim=3, num_classes=1000,\n",
    "\t\tdepths=[2, 3, 8, 2], embed_dims=[64, 128, 256, 320], dim_heads=[32, 32, 32, 32], window_sizes=[7, 7, 7, 7],\n",
    "\t\tkernel_sizes=[3, 3, 3, 3], down_mode='kernel', dilations=[[1], [1], [1, 2, 3], [1, 2]],\n",
    "\t\tnorms=['bn_2d', 'bn_2d', 'bn_2d'], msra_mode='sum', msra_weight=True, msra_norm=True, msra_skip=True,\n",
    "\t\tqkv_bias=True, drop=0., attn_drop=0., drop_path=0.07,\n",
    "\t\top_names=[['conv'], ['conv'], ['msa', 'conv'], ['msa', 'conv']],\n",
    "\t\td_groups=[2, 2, 2, 2], c_groups=[-1, -1, -1, -1], gli_split=True, gli_weight=True,\n",
    "\t\tmlp_ratio=4., cls_head_nums=0, **kwargs)\n",
    "\treturn model\n",
    "\n",
    "\n",
    "def eatformer_small(pretrained=False, **kwargs):\n",
    "\tmodel = EATFormer(\n",
    "\t\t# in_dim=3, num_classes=1000,\n",
    "\t\tdepths=[3, 4, 12, 3], embed_dims=[64, 128, 320, 448], dim_heads=[32, 32, 32, 32], window_sizes=[7, 7, 7, 7],\n",
    "\t\tkernel_sizes=[3, 3, 3, 3], down_mode='kernel', dilations=[[1], [1], [1, 2, 3], [1, 2]],\n",
    "\t\tnorms=['bn_2d', 'bn_2d', 'bn_2d'], msra_mode='sum', msra_weight=True, msra_norm=True, msra_skip=True,\n",
    "\t\tqkv_bias=True, drop=0., attn_drop=0., drop_path=0.10,\n",
    "\t\top_names=[['conv'], ['conv'], ['mdmsa', 'conv'], ['mdmsa', 'conv']],\n",
    "\t\td_groups=[2, 2, 2, 2], c_groups=[-1, -1, -1, -1], gli_split=True, gli_weight=True,\n",
    "\t\tmlp_ratio=4., cls_head_nums=0, **kwargs)\n",
    "\treturn model\n",
    "\n",
    "\n",
    "def eatformer_medium(pretrained=False, **kwargs):\n",
    "\tmodel = EATFormer(\n",
    "\t\t# in_dim=3, num_classes=1000,\n",
    "\t\tdepths=[4, 5, 14, 4], embed_dims=[64, 160, 384, 512], dim_heads=[32, 32, 32, 32], window_sizes=[7, 7, 7, 7],\n",
    "\t\tkernel_sizes=[3, 3, 3, 3], down_mode='kernel', dilations=[[1], [1], [1, 2, 3], [1, 2]],\n",
    "\t\tnorms=['bn_2d', 'bn_2d', 'bn_2d'], msra_mode='sum', msra_weight=True, msra_norm=True, msra_skip=True,\n",
    "\t\tqkv_bias=True, drop=0., attn_drop=0., drop_path=0.2,\n",
    "\t\top_names=[['conv'], ['conv'], ['msa', 'conv'], ['msa', 'conv']],\n",
    "\t\td_groups=[2, 2, 2, 2], c_groups=[-1, -1, -1, -1], gli_split=True, gli_weight=True,\n",
    "\t\tmlp_ratio=4., cls_head_nums=0, **kwargs)\n",
    "\treturn model\n",
    "\n",
    "\n",
    "def eatformer_base(pretrained=False, **kwargs):\n",
    "\tmodel = EATFormer(\n",
    "\t\t# in_dim=3, num_classes=1000,\n",
    "\t\tdepths=[5, 6, 18, 5], embed_dims=[64, 160, 384, 512], dim_heads=[32, 32, 32, 32], window_sizes=[7, 7, 7, 7],\n",
    "\t\tkernel_sizes=[3, 3, 3, 3], down_mode='kernel', dilations=[[1], [1], [1, 2, 3], [1, 2]],\n",
    "\t\tnorms=['bn_2d', 'bn_2d', 'bn_2d'], msra_mode='sum', msra_weight=True, msra_norm=True, msra_skip=True,\n",
    "\t\tqkv_bias=True, drop=0., attn_drop=0., drop_path=0.25,\n",
    "\t\top_names=[['conv'], ['conv'], ['msa', 'conv'], ['msa', 'conv']],\n",
    "\t\td_groups=[2, 2, 2, 2], c_groups=[-1, -1, -1, -1], gli_split=True, gli_weight=True,\n",
    "\t\tmlp_ratio=4., cls_head_nums=0, **kwargs)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34cb6922",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T11:44:54.150183Z",
     "iopub.status.busy": "2025-04-21T11:44:54.149894Z",
     "iopub.status.idle": "2025-04-21T11:44:54.160901Z",
     "shell.execute_reply": "2025-04-21T11:44:54.160343Z"
    },
    "papermill": {
     "duration": 0.015909,
     "end_time": "2025-04-21T11:44:54.162110",
     "exception": false,
     "start_time": "2025-04-21T11:44:54.146201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from timm.scheduler import CosineLRScheduler\n",
    "from PIL import Image\n",
    "\n",
    "# ----------------------------\n",
    "# Dataset Definition\n",
    "# ----------------------------\n",
    "class FaceForensicsDataset(Dataset):\n",
    "    def __init__(self, json_path, root_dir, split, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            json_path (str): Path to the JSON file containing the data split information.\n",
    "            root_dir (str): Base directory for the dataset.\n",
    "            split (str): One of the splits, e.g., 'train', 'val', or 'test'.\n",
    "            transform (callable, optional): A function/transform to apply to the images.\n",
    "        \"\"\"\n",
    "        with open(json_path, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "\n",
    "        # Dynamically iterate over all available categories\n",
    "        for category, category_data in self.data.get(\"FaceForensics++\", {}).items():\n",
    "            try:\n",
    "                videos = category_data[split][\"c23\"]\n",
    "            except KeyError:\n",
    "                print(f\"⚠️ Category {category} does not have the split '{split}' or key 'c23'.\")\n",
    "                continue\n",
    "\n",
    "            for video_id, info in videos.items():\n",
    "                for frame_path in info.get(\"frames\", []):\n",
    "                    frame_path = frame_path.replace(\"\\\\\", \"/\")  # Normalize Windows paths\n",
    "                    if frame_path.startswith(\"FaceForensics++/\"):\n",
    "                        frame_path = \"/\".join(frame_path.split(\"/\")[1:])\n",
    "                    full_path = os.path.join(self.root_dir, frame_path)\n",
    "                    \n",
    "                    if os.path.isfile(full_path):\n",
    "                        label = 1 if category == \"FF-real\" else 0\n",
    "                        self.samples.append((full_path, label))\n",
    "                    else:\n",
    "                        print(f\"⚠️ Skipping missing or invalid file: {full_path}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, torch.tensor(label, dtype=torch.float)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ ERROR loading {img_path}: {e}\")\n",
    "            return torch.zeros(3, 224, 224), torch.tensor(label, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e8b84c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T11:44:54.169309Z",
     "iopub.status.busy": "2025-04-21T11:44:54.169058Z",
     "iopub.status.idle": "2025-04-21T11:44:54.172851Z",
     "shell.execute_reply": "2025-04-21T11:44:54.172050Z"
    },
    "papermill": {
     "duration": 0.008602,
     "end_time": "2025-04-21T11:44:54.173971",
     "exception": false,
     "start_time": "2025-04-21T11:44:54.165369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(epoch, model, optimizer, scaler, best_loss, filename=\"checkpoint.pth\"):\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"scaler_state_dict\": scaler.state_dict(),\n",
    "        \"best_loss\": best_loss\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "    print(f\"✅ Checkpoint saved at epoch {epoch}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc94607d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T11:44:54.180810Z",
     "iopub.status.busy": "2025-04-21T11:44:54.180589Z",
     "iopub.status.idle": "2025-04-21T18:49:49.558001Z",
     "shell.execute_reply": "2025-04-21T18:49:49.556714Z"
    },
    "papermill": {
     "duration": 25495.382915,
     "end_time": "2025-04-21T18:49:49.559986",
     "exception": false,
     "start_time": "2025-04-21T11:44:54.177071",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 GPUs: ['Tesla T4', 'Tesla T4']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-a3789f744f70>:27: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /root/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n",
      "100%|██████████| 30.8M/30.8M [00:00<00:00, 165MB/s] \n",
      "<ipython-input-5-a3789f744f70>:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Checkpoint loaded. Resuming from epoch 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/35 - Training:   0%|          | 0/3591 [00:00<?, ?it/s]<ipython-input-5-a3789f744f70>:91: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/35] - Loss: 0.0033, Accuracy: 99.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/35] - Loss: 0.0033, Accuracy: 99.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/35] - Loss: 0.0029, Accuracy: 99.89%\n",
      "✅ Checkpoint saved at epoch 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/35] - Loss: 0.0035, Accuracy: 99.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/35] - Loss: 0.0032, Accuracy: 99.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/35] - Loss: 0.0034, Accuracy: 99.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/35] - Loss: 0.0026, Accuracy: 99.90%\n",
      "✅ Checkpoint saved at epoch 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/35] - Loss: 0.0031, Accuracy: 99.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/35] - Loss: 0.0026, Accuracy: 99.91%\n",
      "✅ Training Complete. Best model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from timm.scheduler import CosineLRScheduler\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "# ----------------------------\n",
    "# Device Setup\n",
    "# ----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Using {num_gpus} GPUs: {[torch.cuda.get_device_name(i) for i in range(num_gpus)]}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Hyperparameters\n",
    "# ----------------------------\n",
    "batch_size = 32\n",
    "num_epochs = 35\n",
    "learning_rate = 0.01\n",
    "weight_decay = 1e-7\n",
    "start_epoch = 5  # Set start epoch if resuming\n",
    "num_workers = 4\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Model Initialization\n",
    "# ----------------------------\n",
    "model = eatformer_base(pretrained=False, num_classes=1).to(device)\n",
    "if num_gpus > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "# ----------------------------\n",
    "# Loss, Optimizer & Scheduler\n",
    "# ----------------------------\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Data Loaders\n",
    "# ----------------------------\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "])\n",
    "train_dataset = FaceForensicsDataset(json_path=\"/kaggle/input/ff-dd-split/FaceForensics_datasplit.json\", root_dir=\"/kaggle/input/ff-dataset\", split=\"train\", transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "# ----------------------------\n",
    "# Training Loop\n",
    "# ----------------------------\n",
    "best_loss = float('inf')\n",
    "\n",
    "def load_checkpoint(filename=\"checkpoint.pth\"):\n",
    "    if os.path.isfile(filename):\n",
    "        checkpoint = torch.load(filename, map_location=device)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        scaler.load_state_dict(checkpoint[\"scaler_state_dict\"])\n",
    "        start_epoch = checkpoint[\"epoch\"]\n",
    "        best_loss = checkpoint[\"best_loss\"]\n",
    "        print(f\"✅ Checkpoint loaded. Resuming from epoch {start_epoch}\")\n",
    "        return start_epoch, best_loss\n",
    "    else:\n",
    "        print(\"❌ No checkpoint found. Starting from scratch.\")\n",
    "        return 0, float(\"inf\")\n",
    "\n",
    "# Load checkpoint if available\n",
    "start_epoch, best_loss = load_checkpoint(\"/kaggle/input/deat1-30/best_checkpoint.pth\")\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch + 1, num_epochs + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_real_loss = 0.0\n",
    "    running_fake_loss = 0.0\n",
    "    correct, total = 0, 0\n",
    "    train_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs} - Training\", leave=False)\n",
    "    \n",
    "    for images, labels in train_tqdm:\n",
    "        images, labels = images.to(device, non_blocking=True), labels.to(device, non_blocking=True).float().unsqueeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        train_tqdm.set_postfix(loss=loss.item(), acc=100 * correct / total)\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader.dataset)\n",
    "    train_acc = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}] - Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n",
    "    \n",
    "    # Save checkpoint every few epochs or when a new best loss is found\n",
    "    if train_loss < best_loss:\n",
    "        best_loss = train_loss\n",
    "        save_checkpoint(epoch, model, optimizer, scaler, best_loss, \"best_checkpoint.pth\")\n",
    "\n",
    "\n",
    "print(\"✅ Training Complete. Best model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a402f3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T18:49:56.636037Z",
     "iopub.status.busy": "2025-04-21T18:49:56.635702Z",
     "iopub.status.idle": "2025-04-21T18:49:56.639522Z",
     "shell.execute_reply": "2025-04-21T18:49:56.638628Z"
    },
    "papermill": {
     "duration": 3.454464,
     "end_time": "2025-04-21T18:49:56.640962",
     "exception": false,
     "start_time": "2025-04-21T18:49:53.186498",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def load_checkpoint(filename=\"checkpoint.pth\"):\n",
    "#     if os.path.isfile(filename):\n",
    "#         checkpoint = torch.load(filename, map_location=device)\n",
    "#         model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "#         optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "#         scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "#         scaler.load_state_dict(checkpoint[\"scaler_state_dict\"])\n",
    "#         start_epoch = checkpoint[\"epoch\"]\n",
    "#         best_loss = checkpoint[\"best_loss\"]\n",
    "#         print(f\"✅ Checkpoint loaded. Resuming from epoch {start_epoch}\")\n",
    "#         return start_epoch, best_loss\n",
    "#     else:\n",
    "#         print(\"❌ No checkpoint found. Starting from scratch.\")\n",
    "#         return 0, float(\"inf\")\n",
    "\n",
    "# # Load checkpoint if available\n",
    "# start_epoch, best_loss = load_checkpoint(\"/kaggle/input/model0-5/best_checkpoint.pth\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6617877,
     "sourceId": 10682321,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6630764,
     "sourceId": 10700017,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7209754,
     "sourceId": 11500275,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 25532.604186,
   "end_time": "2025-04-21T18:50:03.128534",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-21T11:44:30.524348",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
