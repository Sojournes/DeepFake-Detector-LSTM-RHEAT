{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49a736f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T07:32:40.026257Z",
     "iopub.status.busy": "2025-06-13T07:32:40.025958Z",
     "iopub.status.idle": "2025-06-13T07:32:47.899302Z",
     "shell.execute_reply": "2025-06-13T07:32:47.898229Z"
    },
    "papermill": {
     "duration": 7.88042,
     "end_time": "2025-06-13T07:32:47.901117",
     "exception": false,
     "start_time": "2025-06-13T07:32:40.020697",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred eatformer_binary_classification.pth to /kaggle/working/\n",
      "Transferred losses.json to /kaggle/working/\n",
      "Transferred optimizer_state.pth to /kaggle/working/\n",
      "Transferred scheduler_state.pth to /kaggle/working/\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Define source and destination paths\n",
    "source_path = \"/kaggle/input/eatformer21/\"\n",
    "destination_path = \"/kaggle/working/\"\n",
    "\n",
    "# List of files to transfer\n",
    "files_to_transfer = [\n",
    "    \"eatformer_binary_classification.pth\",\n",
    "    \"losses.json\",\n",
    "    \"optimizer_state.pth\",\n",
    "    \"scheduler_state.pth\"\n",
    "]\n",
    "\n",
    "# Transfer each file\n",
    "for file_name in files_to_transfer:\n",
    "    shutil.copy(f\"{source_path}{file_name}\", destination_path)\n",
    "    print(f\"Transferred {file_name} to {destination_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd5bea2f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-13T07:32:47.910721Z",
     "iopub.status.busy": "2025-06-13T07:32:47.909950Z",
     "iopub.status.idle": "2025-06-13T07:33:08.637937Z",
     "shell.execute_reply": "2025-06-13T07:33:08.636861Z"
    },
    "papermill": {
     "duration": 20.734726,
     "end_time": "2025-06-13T07:33:08.639996",
     "exception": false,
     "start_time": "2025-06-13T07:32:47.905270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting timm==0.6.5\r\n",
      "  Downloading timm-0.6.5-py3-none-any.whl.metadata (45 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.4 in /opt/conda/lib/python3.10/site-packages (from timm==0.6.5) (2.4.0)\r\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from timm==0.6.5) (0.19.0)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->timm==0.6.5) (3.15.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->timm==0.6.5) (4.12.2)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->timm==0.6.5) (1.13.3)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->timm==0.6.5) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->timm==0.6.5) (3.1.4)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.4->timm==0.6.5) (2024.6.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision->timm==0.6.5) (1.26.4)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->timm==0.6.5) (10.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4->timm==0.6.5) (2.1.5)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4->timm==0.6.5) (1.3.0)\r\n",
      "Downloading timm-0.6.5-py3-none-any.whl (512 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.8/512.8 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: timm\r\n",
      "  Attempting uninstall: timm\r\n",
      "    Found existing installation: timm 1.0.9\r\n",
      "    Uninstalling timm-1.0.9:\r\n",
      "      Successfully uninstalled timm-1.0.9\r\n",
      "Successfully installed timm-0.6.5\r\n",
      "Collecting einops\r\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\r\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: einops\r\n",
      "Successfully installed einops-0.8.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install timm==0.6.5\n",
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "006a1655",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T07:33:08.651134Z",
     "iopub.status.busy": "2025-06-13T07:33:08.650819Z",
     "iopub.status.idle": "2025-06-13T07:33:15.855321Z",
     "shell.execute_reply": "2025-06-13T07:33:15.854541Z"
    },
    "papermill": {
     "duration": 7.213045,
     "end_time": "2025-06-13T07:33:15.857638",
     "exception": false,
     "start_time": "2025-06-13T07:33:08.644593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from functools import partial\n",
    "\n",
    "from einops import rearrange, reduce, repeat\n",
    "# from mmcv.ops import DeformConv2d\n",
    "from torchvision.ops import DeformConv2d\n",
    "import torch.nn.functional as F\n",
    "from timm.models.layers.activations import *\n",
    "from timm.models.layers import DropPath, trunc_normal_\n",
    "\n",
    "init_alpha_value = 1e-3\n",
    "init_scale_values = 1e-4\n",
    "\n",
    "\n",
    "# ========== For Common ==========\n",
    "class LayerNormConv(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, normalized_shape, eps=1e-6, elementwise_affine=True):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.norm = nn.LayerNorm(normalized_shape, eps, elementwise_affine)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = rearrange(x, 'b c h w -> b h w c').contiguous()\n",
    "\t\tx = self.norm(x)\n",
    "\t\tx = rearrange(x, 'b h w c -> b c h w').contiguous()\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "def get_norm(norm_layer='in_1d'):\n",
    "\teps = 1e-6\n",
    "\tnorm_dict = {\n",
    "\t\t'none': nn.Identity,\n",
    "\t\t'in_1d': partial(nn.InstanceNorm1d, eps=eps),\n",
    "\t\t'in_2d': partial(nn.InstanceNorm2d, eps=eps),\n",
    "\t\t'in_3d': partial(nn.InstanceNorm3d, eps=eps),\n",
    "\t\t'bn_1d': partial(nn.BatchNorm1d, eps=eps),\n",
    "\t\t'bn_2d': partial(nn.BatchNorm2d, eps=eps),\n",
    "\t\t# 'bn_2d': partial(nn.SyncBatchNorm, eps=eps),\n",
    "\t\t'bn_3d': partial(nn.BatchNorm3d, eps=eps),\n",
    "\t\t'gn': partial(nn.GroupNorm, eps=eps),\n",
    "\t\t'ln': partial(nn.LayerNorm, eps=eps),\n",
    "\t\t'lnc': partial(LayerNormConv, eps=eps),\n",
    "\t}\n",
    "\treturn norm_dict[norm_layer]\n",
    "\n",
    "\n",
    "def get_act(act_layer='relu'):\n",
    "\tact_dict = {\n",
    "\t\t'none': nn.Identity,\n",
    "\t\t'sigmoid': Sigmoid,\n",
    "\t\t'swish': Swish,\n",
    "\t\t'mish': Mish,\n",
    "\t\t'hsigmoid': HardSigmoid,\n",
    "\t\t'hswish': HardSwish,\n",
    "\t\t'hmish': HardMish,\n",
    "\t\t'tanh': Tanh,\n",
    "\t\t'relu': nn.ReLU,\n",
    "\t\t'relu6': nn.ReLU6,\n",
    "\t\t'prelu': PReLU,\n",
    "\t\t'gelu': GELU,\n",
    "\t\t'silu': nn.SiLU\n",
    "\t}\n",
    "\treturn act_dict[act_layer]\n",
    "\n",
    "\n",
    "# ========== Individual ==========\n",
    "class MLP(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, in_dim, hid_dim=None, out_dim=None, act_layer='gelu', drop=0.):\n",
    "\t\tsuper().__init__()\n",
    "\t\tout_dim = out_dim or in_dim\n",
    "\t\thid_dim = hid_dim or in_dim\n",
    "\t\tself.fc1 = nn.Conv2d(in_dim, hid_dim, kernel_size=1, stride=1, padding=0)\n",
    "\t\tself.act = get_act(act_layer)()\n",
    "\t\tself.fc2 = nn.Conv2d(hid_dim, out_dim, kernel_size=1, stride=1, padding=0)\n",
    "\t\tself.drop = nn.Dropout(drop)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.fc1(x)\n",
    "\t\tx = self.act(x)\n",
    "\t\tx = self.drop(x)\n",
    "\t\tx = self.fc2(x)\n",
    "\t\tx = self.drop(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class FFN(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, dim, mlp_ratio=4., drop=0., drop_path=0., act_layer='gelu', norm_layer='lnc'):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.norm = get_norm(norm_layer)(dim)\n",
    "\t\tself.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\t\thid_dim = int(dim * mlp_ratio)\n",
    "\t\tself.mlp = MLP(in_dim=dim, hid_dim=hid_dim, out_dim=dim, act_layer=act_layer, drop=drop)\n",
    "\t\tself.gamma_mlp = nn.Parameter(init_scale_values * torch.ones((dim)), requires_grad=True)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tshortcut = x\n",
    "\t\tx = self.norm(x)\n",
    "\t\tx = shortcut + self.drop_path(self.gamma_mlp.unsqueeze(0).unsqueeze(2).unsqueeze(3) * self.mlp(x))\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "# ========== Global and Local Populations ==========\n",
    "class MSA(nn.Module):\n",
    "\tdef __init__(self, dim, dim_head, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.dim_head = dim_head\n",
    "\t\tself.num_head = dim // dim_head\n",
    "\t\tself.scale = self.dim_head ** -0.5\n",
    "\t\t\n",
    "\t\tself.qkv = nn.Conv2d(dim, dim * 3, kernel_size=1, stride=1, padding=0, bias=qkv_bias)\n",
    "\t\tself.attn_drop = nn.Dropout(attn_drop)\n",
    "\t\tself.proj = nn.Conv2d(dim, dim, kernel_size=1, stride=1, padding=0)\n",
    "\t\tself.proj_drop = nn.Dropout(proj_drop)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tB, C, H, W = x.shape\n",
    "\t\t\n",
    "\t\tqkv = self.qkv(x)\n",
    "\t\tqkv = rearrange(qkv, 'b (qkv heads dim_head) h w -> qkv b heads (h w) dim_head', qkv=3, heads=self.num_head, dim_head=self.dim_head).contiguous()\n",
    "\t\tq, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\t\t\n",
    "\t\tattn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "\t\tattn = attn.softmax(dim=-1)\n",
    "\t\tattn = self.attn_drop(attn)\n",
    "\t\t\n",
    "\t\tx = attn @ v\n",
    "\t\tx = rearrange(x, 'b heads (h w) dim_head -> b (heads dim_head) h w', heads=self.num_head, dim_head=self.dim_head, h=H, w=W).contiguous()\n",
    "\t\tx = self.proj(x)\n",
    "\t\tx = self.proj_drop(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class MSA_OP(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, dim, dim_head, window_size, qkv_bias=False, attn_drop=0., proj_drop=0., init_scale_values=1e-4):\n",
    "\t\tsuper().__init__()\n",
    "\t\tassert dim % dim_head == 0\n",
    "\t\tself.window_size = window_size\n",
    "\t\tself.msa = MSA(dim, dim_head, qkv_bias, attn_drop, proj_drop)\n",
    "\t\tself.gamma_msa = nn.Parameter(init_scale_values * torch.ones((dim)), requires_grad=True)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tB, C, H, W = x.shape\n",
    "\t\tif self.window_size <= 0:\n",
    "\t\t\twindow_size_W, window_size_H = W, H\n",
    "\t\telse:\n",
    "\t\t\twindow_size_W, window_size_H = self.window_size, self.window_size\n",
    "\t\tpad_l, pad_t = 0, 0\n",
    "\t\tpad_r = (window_size_W - W % window_size_W) % window_size_W\n",
    "\t\tpad_b = (window_size_H - H % window_size_H) % window_size_H\n",
    "\t\tx = F.pad(x, (pad_l, pad_r, pad_t, pad_b, 0, 0,))\n",
    "\t\t\n",
    "\t\tn1, n2 = (H + pad_b) // window_size_H, (W + pad_r) // window_size_W\n",
    "\t\tx = rearrange(x, 'b c (h1 n1) (w1 n2) -> (b n1 n2) c h1 w1', n1=n1, n2=n2).contiguous()\n",
    "\t\tx = self.gamma_msa.unsqueeze(0).unsqueeze(2).unsqueeze(3) * self.msa(x)\n",
    "\t\tx = rearrange(x, '(b n1 n2) c h1 w1 -> b c (h1 n1) (w1 n2)', n1=n1, n2=n2).contiguous()\n",
    "\t\t\n",
    "\t\tif pad_r > 0 or pad_b > 0:\n",
    "\t\t\tx = x[:, :, :H, :W].contiguous()\n",
    "\t\t\t\n",
    "\t\treturn x\n",
    "\t\n",
    "\t\n",
    "class DMSA(nn.Module):\n",
    "\tdef __init__(self, dim, dim_head, kernel_size, stride, qkv_bias=False, attn_drop=0., proj_drop=0., d_groups=3):\n",
    "\t\tsuper().__init__()\n",
    "\t\tassert dim % dim_head == 0\n",
    "\t\tself.kernel_size = kernel_size\n",
    "\t\tself.stride = stride\n",
    "\t\tself.dim = dim\n",
    "\t\tself.dim_head = dim_head\n",
    "\t\tself.num_head = dim // dim_head\n",
    "\t\tself.scale = self.dim_head ** -0.5\n",
    "\t\tself.d_groups = d_groups\n",
    "\t\tself.n_group_dim = self.dim // self.d_groups\n",
    "\t\tself.offset_range_factor = 2\n",
    "\t\t\n",
    "\t\tself.conv_offset_modulation = nn.Sequential(\n",
    "\t\t\tnn.Conv2d(self.n_group_dim, self.n_group_dim, self.kernel_size, self.stride, self.kernel_size // 2, groups=self.n_group_dim),\n",
    "\t\t\tget_norm('bn_2d')(self.n_group_dim),\n",
    "\t\t\tnn.GELU(),\n",
    "\t\t\tnn.Conv2d(self.n_group_dim, 3, 1, 1, 0, bias=False)\n",
    "\t\t)\n",
    "\t\tself.modulation_act = get_act('sigmoid')()\n",
    "\t\tself.q = nn.Conv2d(dim, dim * 1, kernel_size=1, stride=1, padding=0, bias=qkv_bias)\n",
    "\t\tself.kv = nn.Conv2d(dim, dim * 2, kernel_size=1, stride=1, padding=0, bias=qkv_bias)\n",
    "\t\tself.proj = nn.Conv2d(dim, dim, kernel_size=1, stride=1, padding=0, bias=True)\n",
    "\t\tself.attn_drop = nn.Dropout(attn_drop)\n",
    "\t\tself.proj_drop = nn.Dropout(proj_drop)\n",
    "\t\n",
    "\t@torch.no_grad()\n",
    "\tdef _get_ref_points(self, H, W, B, dtype, device):\n",
    "\t\tref_y, ref_x = torch.meshgrid(torch.linspace(0.5, H - 0.5, H, dtype=dtype, device=device),\n",
    "\t\t\t\t\t\t\t\t\t  torch.linspace(0.5, W - 0.5, W, dtype=dtype, device=device))\n",
    "\t\tref = torch.stack((ref_y, ref_x), -1)\n",
    "\t\tref[..., 1].div_(W).mul_(2).sub_(1)\n",
    "\t\tref[..., 0].div_(H).mul_(2).sub_(1)\n",
    "\t\tref = ref[None, ...].expand(B * self.d_groups, -1, -1, -1)  # B * g H W 2\n",
    "\t\treturn ref\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tB, C, H, W = x.shape\n",
    "\t\tq = self.q(x)\n",
    "\t\tq_off = rearrange(q, 'b (g c) h w -> (b g) c h w', g=self.d_groups, c=self.n_group_dim).contiguous()\n",
    "\t\toffset_modulation = self.conv_offset_modulation(q_off)  # bg 3 h w\n",
    "\t\toffset, modulation = offset_modulation[:, 0:2, :, :], self.modulation_act(offset_modulation[:, 2:3, :, :])  # bg 2 h w, bg 1 h w\n",
    "\t\tH_off, W_off = offset.size(2), offset.size(3)\n",
    "\t\t\n",
    "\t\toffset_range = torch.tensor([1.0 / H_off, 1.0 / W_off], device=x.device).reshape(1, 2, 1, 1)\n",
    "\t\toffset = offset.tanh().mul(offset_range).mul(self.offset_range_factor)\n",
    "\t\toffset = rearrange(offset, 'b c h w -> b h w c').contiguous()\n",
    "\t\treference = self._get_ref_points(H_off, W_off, B, x.dtype, x.device)\n",
    "\t\tpos = offset + reference\n",
    "\t\t\n",
    "\t\tx_sampled = F.grid_sample(input=x.reshape(B * self.d_groups, self.n_group_dim, H, W),\n",
    "\t\t\t\t\t\t\t\t  grid=pos[..., (1, 0)],  # y, x -> x, y\n",
    "\t\t\t\t\t\t\t\t  mode='bilinear', align_corners=True)  # B * g, Cg, Hg, Wg\n",
    "\t\tx_sampled *= modulation.sigmoid()\n",
    "\t\tx_sampled = rearrange(x_sampled, '(b g) c h w -> b (g c) h w', b=B, g=self.d_groups).contiguous()\n",
    "\t\tq = rearrange(q, 'b (heads dim_head) h w -> b heads (h w) dim_head', heads=self.num_head,\n",
    "\t\t\t\t\t  dim_head=self.dim_head).contiguous()\n",
    "\t\tkv = self.kv(x_sampled)\n",
    "\t\tkv = rearrange(kv, 'b (kv heads dim_head) h w -> kv b heads (h w) dim_head', kv=2, heads=self.num_head,\n",
    "\t\t\t\t\t   dim_head=self.dim_head).contiguous()\n",
    "\t\tk, v = kv[0], kv[1]\n",
    "\t\t\n",
    "\t\tattn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "\t\tattn = attn.softmax(dim=-1)\n",
    "\t\tattn = self.attn_drop(attn)\n",
    "\t\t\n",
    "\t\tx = attn @ v\n",
    "\t\tx = rearrange(x, 'b heads (h w) dim_head -> b (heads dim_head) h w', heads=self.num_head,\n",
    "\t\t\t\t\t  dim_head=self.dim_head, h=H, w=W).contiguous()\n",
    "\t\tx = self.proj(x)\n",
    "\t\tx = self.proj_drop(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class DMSA_OP(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, dim, dim_head, window_size, kernel_size, stride, qkv_bias=False, attn_drop=0., proj_drop=0., d_groups=3):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.window_size = window_size\n",
    "\t\tself.mdmsa = DMSA(dim, dim_head, kernel_size, stride, qkv_bias, attn_drop, proj_drop, d_groups)\n",
    "\t\tself.gamma_mdmsa = nn.Parameter(init_scale_values * torch.ones((dim)), requires_grad=True)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tB, C, H, W = x.shape\n",
    "\t\tif self.window_size <= 0:\n",
    "\t\t\twindow_size_W, window_size_H = W, H\n",
    "\t\telse:\n",
    "\t\t\twindow_size_W, window_size_H = self.window_size, self.window_size\n",
    "\t\tpad_l, pad_t = 0, 0\n",
    "\t\tpad_r = (window_size_W - W % window_size_W) % window_size_W\n",
    "\t\tpad_b = (window_size_H - H % window_size_H) % window_size_H\n",
    "\t\tx = F.pad(x, (pad_l, pad_r, pad_t, pad_b, 0, 0,))\n",
    "\t\t\n",
    "\t\tn1, n2 = (H + pad_b) // window_size_H, (W + pad_r) // window_size_W\n",
    "\t\tx = rearrange(x, 'b c (h1 n1) (w1 n2) -> (b n1 n2) c h1 w1', n1=n1, n2=n2).contiguous()\n",
    "\t\tx = self.gamma_mdmsa.unsqueeze(0).unsqueeze(2).unsqueeze(3) * self.mdmsa(x)\n",
    "\t\tx = rearrange(x, '(b n1 n2) c h1 w1 -> b c (h1 n1) (w1 n2)', n1=n1, n2=n2).contiguous()\n",
    "\t\t\n",
    "\t\tif pad_r > 0 or pad_b > 0:\n",
    "\t\t\tx = x[:, :, :H, :W].contiguous()\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class Conv_OP(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, dim, kernel_size, stride=1):\n",
    "\t\tsuper().__init__()\n",
    "\t\tpadding = math.ceil((kernel_size - stride) / 2)\n",
    "\t\tself.conv1 = nn.Conv2d(dim, dim, kernel_size, stride, padding, groups=dim)\n",
    "\t\tself.norm1 = get_norm('bn_2d')(dim)\n",
    "\t\tself.act1 = get_act('silu')()\n",
    "\t\tself.conv2 = nn.Conv2d(dim, dim, 1, 1, 0)\n",
    "\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.conv1(x)\n",
    "\t\tx = self.norm1(x)\n",
    "\t\tx = self.act1(x)\n",
    "\t\tx = self.conv2(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class DCN2_OP(nn.Module):\n",
    "\t# ref: https://github.com/WenmuZhou/DBNet.pytorch/blob/678b2ae55e018c6c16d5ac182558517a154a91ed/models/backbone/resnet.py\n",
    "\tdef __init__(self, dim, kernel_size=3, stride=1, deform_groups=4):\n",
    "\t\tsuper().__init__()\n",
    "\t\toffset_channels = kernel_size * kernel_size * 2\n",
    "\t\tself.conv1_offset = nn.Conv2d(dim, deform_groups * offset_channels, kernel_size=3, stride=stride, padding=1)\n",
    "\t\tself.conv1 = DeformConv2d(dim, dim, kernel_size=kernel_size, padding=kernel_size // 2, bias=False)\n",
    "\t\tself.norm1 = get_norm('bn_2d')(dim)\n",
    "\t\tself.act1 = get_act('silu')()\n",
    "\t\tself.conv2 = nn.Conv2d(dim, dim, 1, 1, 0)\n",
    "\t\t\n",
    "\tdef forward(self, x):\n",
    "\t\toffset = self.conv1_offset(x)\n",
    "\t\tx = self.conv1(x, offset)\n",
    "\t\tx = self.norm1(x)\n",
    "\t\tx = self.act1(x)\n",
    "\t\tx = self.conv2(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class GLI(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, in_dim, dim_head, window_size, kernel_size=5, qkv_bias=False, drop=0., attn_drop=0.,\n",
    "\t\t\t\t drop_path=0., act_layer='gelu', norm_layer='bn_2d',\n",
    "\t\t\t\t op_names=['msa', 'mdmsa', 'conv', 'dcn'], d_group=3, gli_split=False, gli_weight=True, gli_ratio=None):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.op_names = op_names\n",
    "\t\tself.gli_split = gli_split\n",
    "\t\tself.gli_weight = gli_weight\n",
    "\t\tself.gli_ratio = gli_ratio\n",
    "\t\tself.op_num = len(op_names)\n",
    "\t\tself.norm = get_norm(norm_layer)(in_dim)\n",
    "\t\tself.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\t\tif self.op_num == 1:\n",
    "\t\t\tdims = [in_dim]\n",
    "\t\telse:\n",
    "\t\t\tif gli_split:\n",
    "\t\t\t\tif gli_ratio:\n",
    "\t\t\t\t\tassert self.op_num == 2\n",
    "\t\t\t\t\tdims = [int(in_dim * gli_ratio), round(in_dim * (1 - gli_ratio))]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tdim = in_dim // self.op_num\n",
    "\t\t\t\t\tassert dim * self.op_num == in_dim\n",
    "\t\t\t\t\tdims = [dim] * self.op_num\n",
    "\t\t\telse:\n",
    "\t\t\t\tdims = [in_dim] * self.op_num\n",
    "\t\tself.dims = dims\n",
    "\t\tself.ops = nn.ModuleList()\n",
    "\t\tfor idx, op_name in enumerate(op_names):\n",
    "\t\t\tif op_name in ['conv', 'c']:\n",
    "\t\t\t\top = Conv_OP(dims[idx], kernel_size, stride=1)\n",
    "\t\t\telif op_name in ['dcn', 'dc']:\n",
    "\t\t\t\top = DCN2_OP(dims[idx], kernel_size, stride=1, deform_groups=d_group)\n",
    "\t\t\telif op_name in ['msa', 'm']:\n",
    "\t\t\t\top = MSA_OP(dims[idx], dim_head, window_size, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n",
    "\t\t\telif op_name in ['mdmsa', 'dm']:\n",
    "\t\t\t\top = DMSA_OP(dims[idx], dim_head, window_size, kernel_size=5, stride=1, qkv_bias=True, attn_drop=attn_drop, proj_drop=drop, d_groups=d_group)\n",
    "\t\t\telse:\n",
    "\t\t\t\traise 'invalid \\'{}\\' operation'.format(op_name)\n",
    "\t\t\tself.ops.append(op)\n",
    "\t\tif self.op_num > 1 and gli_weight:\n",
    "\t\t\tself.alphas = nn.Parameter(init_alpha_value * torch.ones(self.op_num), requires_grad=True)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tshortcut = x\n",
    "\t\tx = self.norm(x)\n",
    "\t\tif self.op_num == 1:\n",
    "\t\t\tx = self.ops[0](x)\n",
    "\t\telse:\n",
    "\t\t\tif self.gli_split:\n",
    "\t\t\t\tif self.gli_ratio:\n",
    "\t\t\t\t\txs = [x[:, :self.dims[0], :, :], x[:, self.dims[0]:, :, :]]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\txs = torch.chunk(x, self.op_num, dim=1)\n",
    "\t\t\telse:\n",
    "\t\t\t\txs = [x] * self.op_num\n",
    "\t\t\tif self.gli_weight:\n",
    "\t\t\t\talphas = F.softmax(self.alphas, dim=-1)\n",
    "\t\t\t\tif self.gli_split:\n",
    "\t\t\t\t\tif self.gli_ratio:\n",
    "\t\t\t\t\t\tx = torch.cat([self.ops[i](xs[i]) * alphas[i] for i in range(self.op_num)], dim=1).contiguous()\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\txs = torch.cat([self.ops[i](xs[i]).unsqueeze(dim=-1) * alphas[i] for i in range(self.op_num)], dim=-1)\n",
    "\t\t\t\t\t\tx = rearrange(xs, 'b c h w n -> b (c n) h w').contiguous()\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\txs = torch.cat([self.ops[i](xs[i]).unsqueeze(dim=-1) * alphas[i] for i in range(self.op_num)], dim=-1)\n",
    "\t\t\t\t\tx = reduce(xs, 'b c h w n -> b c h w', 'mean').contiguous()\n",
    "\t\t\telse:\n",
    "\t\t\t\tif self.gli_split:\n",
    "\t\t\t\t\tx = torch.cat([self.ops[i](xs[i]) for i in range(self.op_num)], dim=1)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\txs = torch.cat([self.ops[i](xs[i]).unsqueeze(dim=-1) for i in range(self.op_num)], dim=-1)\n",
    "\t\t\t\t\tx = reduce(xs, 'b c h w n -> b c h w', 'mean').contiguous()\n",
    "\t\tx = shortcut + self.drop_path(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "# ========== Multi-Scale Populations ==========\n",
    "class MSP(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, in_dim, emb_dim, kernel_size=3, c_group=-1, stride=1, dilations=[1, 2, 3], msra_mode='cat',\n",
    "\t\t\t\t act_layer='silu', norm_layer='bn_2d', msra_weight=True):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.msra_mode = msra_mode\n",
    "\t\tself.msra_weight = msra_weight\n",
    "\t\tself.dilation_num = len(dilations)\n",
    "\t\tassert in_dim % c_group == 0\n",
    "\t\tc_group = (in_dim if c_group == -1 else c_group) if stride == 1 else 1\n",
    "\t\tself.convs = nn.ModuleList()\n",
    "\t\tfor i in range(len(dilations)):\n",
    "\t\t\tpadding = math.ceil(((kernel_size - 1) * dilations[i] + 1 - stride) / 2)\n",
    "\t\t\tself.convs.append(nn.Sequential(\n",
    "\t\t\t\tnn.Conv2d(in_dim, emb_dim, kernel_size, stride, padding, dilations[i], groups=c_group),\n",
    "\t\t\t\tget_act(act_layer)(emb_dim)))\n",
    "\t\tif self.dilation_num > 1 and msra_weight:\n",
    "\t\t\tself.alphas = nn.Parameter(init_alpha_value * torch.ones(self.dilation_num), requires_grad=True)\n",
    "\t\tself.conv_out = nn.Conv2d(emb_dim * (self.dilation_num if msra_mode == 'cat' else 1), emb_dim, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\t# B, C, H, W\n",
    "\t\tif self.dilation_num == 1:\n",
    "\t\t\tx = self.convs[0](x)\n",
    "\t\telse:\n",
    "\t\t\tif self.msra_weight:\n",
    "\t\t\t\talphas = F.softmax(self.alphas, dim=-1)\n",
    "\t\t\t\tx = torch.cat([self.convs[i](x).unsqueeze(dim=-1) * alphas[i] for i in range(self.dilation_num)], dim=-1)\n",
    "\t\t\telse:\n",
    "\t\t\t\tx = torch.cat([self.convs[i](x).unsqueeze(dim=-1) for i in range(self.dilation_num)], dim=-1)\n",
    "\t\t\tif self.msra_mode == 'cat':\n",
    "\t\t\t\tx = rearrange(x, 'b c h w n -> b (c n) h w').contiguous()\n",
    "\t\t\telif self.msra_mode == 'sum':\n",
    "\t\t\t\tx = reduce(x, 'b c h w n -> b c h w', 'mean').contiguous()\n",
    "\t\tx = self.conv_out(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class MSRA(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, in_dim, emb_dim, kernel_size=3, c_group=-1, stride=1, dilations=[1, 2, 3], msra_mode='cat',\n",
    "\t\t\t\t act_layer='silu', norm_layer='bn_2d', msra_weight=True, msra_norm=True, msra_skip=True, drop_path=0.):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.norm = get_norm(norm_layer)(in_dim) if msra_norm else nn.Identity()\n",
    "\t\tself.msp = MSP(in_dim, emb_dim, kernel_size, c_group, stride, dilations, msra_mode, act_layer, norm_layer, msra_weight)\n",
    "\t\tself.msra_skip = msra_skip\n",
    "\t\tif msra_skip:\n",
    "\t\t\tif stride == 1:\n",
    "\t\t\t\tself.skip_conv = nn.Identity()\n",
    "\t\t\telse:\n",
    "\t\t\t\tself.skip_conv = nn.Sequential(nn.AvgPool2d(kernel_size=stride, stride=stride, ceil_mode=True, count_include_pad=False),\n",
    "\t\t\t\t\t\t\t\t\t\t\t   nn.Conv2d(in_dim, emb_dim, 1, stride=1, padding=0, bias=False),\n",
    "\t\t\t\t\t\t\t\t\t\t\t   get_norm(norm_layer)(emb_dim))\n",
    "\t\t\tself.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tshortcut = x\n",
    "\t\tx = self.msp(self.norm(x))\n",
    "\t\tif self.msra_skip:\n",
    "\t\t\tx = self.skip_conv(shortcut) + self.drop_path(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "# ========== Block ==========\n",
    "class EATBlock(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, in_dim, emb_dim, kernel_size=3, stride=1, dilations=[1, 2, 3], norms=['bn_2d', 'bn_2d', 'bn_2d'],\n",
    "\t\t\t\t msra_mode='cat', msra_weight=True, msra_norm=True, msra_skip=True,\n",
    "\t\t\t\t dim_head=6, window_size=7, qkv_bias=True, drop=0., attn_drop=0., drop_path=0.,\n",
    "\t\t\t\t op_names=['msa', 'conv'], d_group=3, c_group=-1, gli_split=False, gli_weight=True, gli_ratio=None, mlp_ratio=4., ):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.layer1 = MSRA(in_dim, emb_dim, kernel_size, c_group, stride, dilations, msra_mode, 'silu', norms[0],\n",
    "\t\t\t\t\t\t\t\t\tmsra_weight, msra_norm, msra_skip, drop_path)\n",
    "\t\tself.layer2 = GLI(emb_dim, dim_head, window_size, 5, qkv_bias,\n",
    "\t\t\t\t\t\t\t\t   drop, attn_drop, drop_path, 'silu', norms[1],\n",
    "\t\t\t\t\t\t\t\t   op_names, d_group, gli_split, gli_weight, gli_ratio)\n",
    "\t\tself.layer3 = FFN(emb_dim, mlp_ratio, drop, drop_path, 'gelu', norms[2])\n",
    "\t\n",
    "\tdef forward(self, x):\n",
    "\t\tx = self.layer1(x)\n",
    "\t\tx = self.layer2(x)\n",
    "\t\tx = self.layer3(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "# ========== Task-related Head ==========\n",
    "class MCA(nn.Module):\n",
    "\tdef __init__(self, dim, dim_head=8, qkv_bias=False, attn_drop=0., proj_drop=0.):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.dim_head = dim_head\n",
    "\t\tself.num_head = dim // dim_head\n",
    "\t\tself.scale = self.dim_head ** -0.5\n",
    "\t\t\n",
    "\t\tself.q = nn.Conv2d(dim, dim * 1, kernel_size=1, stride=1, padding=0, bias=qkv_bias)\n",
    "\t\tself.kv = nn.Conv2d(dim, dim * 2, kernel_size=1, stride=1, padding=0, bias=qkv_bias)\n",
    "\t\tself.attn_drop = nn.Dropout(attn_drop)\n",
    "\t\tself.proj = nn.Conv2d(dim, dim, kernel_size=1, stride=1, padding=0, )\n",
    "\t\tself.proj_drop = nn.Dropout(proj_drop)\n",
    "\t\n",
    "\tdef forward(self, x, xq):\n",
    "\t\tB, C, H, W = x.shape\n",
    "\t\t_, _, Hq, Wq = xq.shape\n",
    "\t\t\n",
    "\t\tq = self.q(xq)\n",
    "\t\tkv = self.kv(x)\n",
    "\t\tq = rearrange(q, 'b (q heads dim_head) h w -> q b heads (h w) dim_head', q=1, heads=self.num_head,\n",
    "\t\t\t\t\t  dim_head=self.dim_head).contiguous()\n",
    "\t\tkv = rearrange(kv, 'b (kv heads dim_head) h w -> kv b heads (h w) dim_head', kv=2, heads=self.num_head,\n",
    "\t\t\t\t\t   dim_head=self.dim_head).contiguous()\n",
    "\t\tq, k, v = q[0], kv[0], kv[1]\n",
    "\t\t\n",
    "\t\tattn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "\t\tattn = attn.softmax(dim=-1)\n",
    "\t\tattn = self.attn_drop(attn)\n",
    "\t\t\n",
    "\t\tx = attn @ v\n",
    "\t\tx = rearrange(x, 'b heads (h w) dim_head -> b (heads dim_head) h w', heads=self.num_head,\n",
    "\t\t\t\t\t  dim_head=self.dim_head, h=Hq, w=Wq).contiguous()\n",
    "\t\tx = self.proj(x)\n",
    "\t\tx = self.proj_drop(x)\n",
    "\t\treturn x\n",
    "\n",
    "\n",
    "class TRHead(nn.Module):\n",
    "\t\n",
    "\tdef __init__(self, dim, dim_head, mlp_ratio=4., qkv_bias=False, drop=0., attn_drop=0.,\n",
    "\t\t\t\t drop_path=0., act_layer='gelu', norm_layer='lnc'):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.norm_kv = get_norm(norm_layer)(dim)\n",
    "\t\tself.norm1 = get_norm(norm_layer)(dim)\n",
    "\t\tself.attn = MCA(dim, dim_head=dim_head, qkv_bias=qkv_bias, attn_drop=attn_drop, proj_drop=drop)\n",
    "\t\tself.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\t\tself.norm2 = get_norm(norm_layer)(dim)\n",
    "\t\thid_dim = int(dim * mlp_ratio)\n",
    "\t\tself.mlp = MLP(in_dim=dim, hid_dim=hid_dim, out_dim=dim, act_layer=act_layer, drop=drop)\n",
    "\t\n",
    "\tdef forward(self, x, xq):\n",
    "\t\txq = xq + self.drop_path(self.attn(self.norm_kv(x), self.norm1(xq)))\n",
    "\t\txq = xq + self.drop_path(self.mlp(self.norm2(xq)))\n",
    "\t\treturn xq\n",
    "\n",
    "\n",
    "class HaarWaveletTransform(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(HaarWaveletTransform, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        # Projection layer to match the number of channels if necessary\n",
    "        if in_channels != out_channels:\n",
    "            self.proj = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        else:\n",
    "            self.proj = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, channels, height, width)\n",
    "        batch_size, channels, height, width = x.size()\n",
    "\n",
    "        # Ensure input dimensions are even\n",
    "        if height % 2 != 0 or width % 2 != 0:\n",
    "            pad_h = 1 if height % 2 != 0 else 0\n",
    "            pad_w = 1 if width % 2 != 0 else 0\n",
    "            x = nn.functional.pad(x, (0, pad_w, 0, pad_h))\n",
    "            height += pad_h\n",
    "            width += pad_w\n",
    "\n",
    "        # Apply Haar wavelet transform on each channel\n",
    "        transformed = []\n",
    "        for c in range(channels):\n",
    "            # Convert tensor to numpy\n",
    "            img = x[:, c, :, :].cpu().detach().numpy()\n",
    "\n",
    "            # Apply DWT2\n",
    "            coeffs = pywt.dwt2(img, 'haar')\n",
    "            LL, (LH, HL, HH) = coeffs\n",
    "\n",
    "            # Compute the Haar feature (XRC_cap)\n",
    "            XRC_cap = np.sqrt(LH**2 + HL**2 + HH**2)\n",
    "\n",
    "            # Convert back to tensor\n",
    "            XRC_cap_tensor = torch.tensor(XRC_cap, dtype=x.dtype).to(x.device)\n",
    "            transformed.append(XRC_cap_tensor.unsqueeze(1))  # (batch, 1, H/2, W/2)\n",
    "\n",
    "        # Stack all channels\n",
    "        transformed = torch.cat(transformed, dim=1)  # (batch, channels, H/2, W/2)\n",
    "\n",
    "        # Upsample to match original spatial dimensions\n",
    "        transformed = nn.functional.interpolate(transformed, size=(height, width), mode='bilinear', align_corners=False)\n",
    "\n",
    "        # Project to match output channels\n",
    "        transformed = self.proj(transformed)\n",
    "\n",
    "        return transformed\n",
    "\n",
    "class EATFormer(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim=3, num_classes=1000,\n",
    "                 depths=[2, 2, 6, 2], embed_dims=[64, 128, 256, 512], dim_heads=[32, 32, 32, 32],\n",
    "                 window_sizes=[7, 7, 7, 7], kernel_sizes=[3, 3, 3, 3], down_mode='kernel',\n",
    "                 dilations=[[1], [1], [1, 2, 3], [1, 2]], norms=['bn_2d', 'bn_2d', 'bn_2d'],\n",
    "                 msra_mode='sum', msra_weight=True, msra_norm=True, msra_skip=True,\n",
    "                 qkv_bias=True, drop=0., attn_drop=0., drop_path=0.,\n",
    "                 op_names=[['conv'], ['conv'], ['msa', 'conv'], ['msa', 'conv']],\n",
    "                 d_groups=[3, 3, 3, 3], c_groups=[-1, -1, -1, -1], gli_split=False, gli_weight=True, gli_ratio=None,\n",
    "                 mlp_ratio=4., cls_head_nums=0):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.cls_head_nums = cls_head_nums\n",
    "        dprs = [x.item() for x in torch.linspace(0, drop_path, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "        # Stage 0: Initial convolutional blocks\n",
    "        self.stage0 = nn.ModuleList([\n",
    "\t\t\tMSRA(in_dim, embed_dims[0] // 2, kernel_size=3, c_group=1, stride=2, dilations=[1], msra_mode='sum',\n",
    "\t\t\t\t\t\t  act_layer='silu', norm_layer='bn_2d', msra_weight=False,\n",
    "\t\t\t\t\t\t  msra_norm=False, msra_skip=False),\n",
    "\t\t\tMSRA(embed_dims[0] // 2, embed_dims[0], kernel_size=3, c_group=1, stride=2, dilations=[1], msra_mode='sum',\n",
    "\t\t\t\t\t\t  act_layer='silu', norm_layer='bn_2d', msra_weight=False,\n",
    "\t\t\t\t\t\t  msra_norm=True, msra_skip=False)\n",
    "        ])\n",
    "\n",
    "        # Haar Wavelet Transform as Residual\n",
    "        self.haar_residual = nn.Conv2d(3, 64, kernel_size=1)  # Haar transform to generate 64 channels\n",
    "\n",
    "        \n",
    "         # Initialize a convolution layer for adjusting haar_features size\n",
    "        self.adjust_conv = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=1)  # Adjust to 64 channels\n",
    "\n",
    "\n",
    "        # Build stages (stage1, stage2, etc.)\n",
    "        emb_dim_pre = embed_dims[0]\n",
    "        for i in range(len(depths)):\n",
    "            layers = []\n",
    "            dpr = dprs[sum(depths[:i]):sum(depths[:i + 1])]\n",
    "            for j in range(depths[i]):\n",
    "                stride = 2 if j == 0 and i > 0 else 1\n",
    "                kernel_size = stride if stride > 1 and down_mode == 'patch' else kernel_sizes[i]\n",
    "                layers.append(EATBlock(emb_dim_pre, emb_dim=embed_dims[i], kernel_size=kernel_size, stride=stride,\n",
    "                                      dilations=dilations[i], norms=norms, msra_mode=msra_mode, msra_weight=msra_weight, msra_norm=msra_norm,\n",
    "                                      msra_skip=msra_skip, dim_head=dim_heads[i], window_size=window_sizes[i],\n",
    "                                      qkv_bias=qkv_bias, drop=drop, attn_drop=attn_drop, drop_path=dpr[j],\n",
    "                                      op_names=op_names[i], d_group=d_groups[i], c_group=c_groups[i],\n",
    "                                      gli_split=gli_split, gli_weight=gli_weight, gli_ratio=gli_ratio, mlp_ratio=mlp_ratio,))\n",
    "                emb_dim_pre = embed_dims[i]\n",
    "            self.__setattr__(f'stage{i + 1}', nn.ModuleList(layers))\n",
    "\n",
    "        # Classification head\n",
    "        if cls_head_nums:\n",
    "            self.cls_token = nn.Parameter(torch.zeros(1, embed_dims[-1], 1, 1))\n",
    "            layers = [TRHead(embed_dims[-1], dim_heads[-1], mlp_ratio=mlp_ratio,\n",
    "                             qkv_bias=qkv_bias, drop=drop, attn_drop=attn_drop, drop_path=0.)\n",
    "                      for _ in range(cls_head_nums)]\n",
    "            self.stage_cls = nn.ModuleList(layers)\n",
    "        else:\n",
    "            self.cls_token, self.stage_cls = None, nn.ModuleList()\n",
    "\n",
    "        self.norm = nn.BatchNorm2d(embed_dims[-1])\n",
    "        self.head = nn.Linear(embed_dims[-1], num_classes) if num_classes > 0 else nn.Identity()\n",
    "        \n",
    "        if self.cls_token is not None:\n",
    "            trunc_normal_(self.cls_token, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token'}\n",
    "    \n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {'alpha', 'gamma', 'beta'}\n",
    "    \n",
    "    def no_ft_keywords(self):\n",
    "        return {}\n",
    "    \n",
    "    def ft_head_keywords(self):\n",
    "        return {'head.weight', 'head.bias'}, self.num_classes\n",
    "    \n",
    "    def get_classifier(self):\n",
    "        return self.head\n",
    "    \n",
    "    def reset_classifier(self, num_classes, global_pool=''):\n",
    "        self.num_classes = num_classes\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "    \n",
    "    def check_bn(self):\n",
    "        for name, m in self.named_modules():\n",
    "            if isinstance(m, torch.nn.modules.batchnorm._NormBase):\n",
    "                m.running_mean.nan_to_num_(nan=0, posinf=1, neginf=-1)\n",
    "                m.running_var.nan_to_num_(nan=0, posinf=1, neginf=-1)\n",
    "\n",
    "    def adjust_haar_size(self, haar_features, target_size):\n",
    "        # Resize the haar_features using an interpolation method or additional conv layer\n",
    "        # Use the already initialized self.adjust_conv layer\n",
    "        haar_features = self.adjust_conv(haar_features) \n",
    "\n",
    "        # Now ensure it matches the target size using interpolation\n",
    "        if haar_features.size(2) != target_size[2] or haar_features.size(3) != target_size[3]:\n",
    "            haar_features = nn.functional.interpolate(haar_features, size=(target_size[2], target_size[3]), mode='bilinear', align_corners=False)\n",
    "\n",
    "        return haar_features\n",
    "    \n",
    "    def forward_features(self, x):\n",
    "        # Apply Haar wavelet transform before stage0\n",
    "        haar_features = self.haar_residual(x)\n",
    "\n",
    "        # Pass through stage0 convolution blocks\n",
    "        for blk in self.stage0:\n",
    "            x = blk(x)\n",
    "\n",
    "        # If needed, use a convolution to match sizes\n",
    "        if haar_features.size() != x.size():\n",
    "            haar_features = self.adjust_haar_size(haar_features, x.size())\n",
    "\n",
    "        # Introduce learnable weight for combining Haar and original features\n",
    "        alpha = nn.Parameter(torch.ones(1, device=x.device))  # Initialize alpha on the same device as x\n",
    "\n",
    "        # Perform the weighted sum of x and haar_features\n",
    "        x = alpha * x + (1 - alpha) * haar_features\n",
    "\n",
    "        # Pass through remaining stages\n",
    "        for blk in self.stage1:\n",
    "            x = blk(x)\n",
    "        for blk in self.stage2:\n",
    "            x = blk(x)\n",
    "        for blk in self.stage3:\n",
    "            x = blk(x)\n",
    "        for blk in self.stage4:\n",
    "            x = blk(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        x = self.forward_features(x)\n",
    "        if self.cls_head_nums:\n",
    "            cls_token = self.cls_token.expand(B, -1, -1, -1)\n",
    "            for blk in self.stage_cls:\n",
    "                cls_token = blk(x, cls_token)\n",
    "            x = cls_token\n",
    "            \n",
    "        x = self.norm(x)\n",
    "        x = reduce(x, 'b c h w -> b c', 'mean').contiguous()\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "def eatformer_mobile(pretrained=False, **kwargs):\n",
    "\tmodel = EATFormer(\n",
    "\t\t# in_dim=3, num_classes=1000,\n",
    "\t\tdepths=[1, 1, 4, 1], embed_dims=[48, 64, 160, 256], dim_heads=[16, 16, 20, 32], window_sizes=[7, 7, 7, 7],\n",
    "\t\tkernel_sizes=[3, 3, 3, 3], down_mode='patch', dilations=[[1], [1], [1, 2], [1]],\n",
    "\t\tnorms=['bn_2d', 'bn_2d', 'bn_2d'], msra_mode='sum', msra_weight=True, msra_norm=True, msra_skip=True,\n",
    "\t\tqkv_bias=True, drop=0., attn_drop=0., drop_path=0.03,\n",
    "\t\top_names=[['conv'], ['conv'], ['mdmsa', 'conv'], ['mdmsa', 'conv']],\n",
    "\t\td_groups=[2, 2, 2, 2], c_groups=[-1, -1, -1, -1], gli_split=True, gli_weight=True,\n",
    "\t\tmlp_ratio=2.5, cls_head_nums=0, **kwargs)\n",
    "\treturn model\n",
    "\n",
    "\n",
    "def eatformer_lite(pretrained=False, **kwargs):\n",
    "\tmodel = EATFormer(\n",
    "\t\t# in_dim=3, num_classes=1000,\n",
    "\t\tdepths=[1, 2, 6, 1], embed_dims=[64, 128, 192, 256], dim_heads=[32, 32, 32, 32], window_sizes=[7, 7, 7, 7],\n",
    "\t\tkernel_sizes=[3, 3, 3, 3], down_mode='patch', dilations=[[1], [1], [1, 2], [1]],\n",
    "\t\tnorms=['bn_2d', 'bn_2d', 'bn_2d'], msra_mode='sum', msra_weight=True, msra_norm=True, msra_skip=True,\n",
    "\t\tqkv_bias=True, drop=0., attn_drop=0., drop_path=0.03,\n",
    "\t\top_names=[['conv'], ['conv'], ['mdmsa', 'conv'], ['mdmsa', 'conv']],\n",
    "\t\td_groups=[2, 2, 2, 2], c_groups=[-1, -1, -1, -1], gli_split=True, gli_weight=True,\n",
    "\t\tmlp_ratio=3, cls_head_nums=0, **kwargs)\n",
    "\treturn model\n",
    "\n",
    "\n",
    "def eatformer_tiny(pretrained=False, **kwargs):\n",
    "\tmodel = EATFormer(\n",
    "\t\t# in_dim=3, num_classes=1000,\n",
    "\t\tdepths=[2, 2, 6, 2], embed_dims=[64, 128, 192, 256], dim_heads=[32, 32, 32, 32], window_sizes=[7, 7, 7, 7],\n",
    "\t\tkernel_sizes=[3, 3, 3, 3], down_mode='kernel', dilations=[[1], [1], [1, 2, 3], [1, 2]],\n",
    "\t\tnorms=['bn_2d', 'bn_2d', 'bn_2d'], msra_mode='sum', msra_weight=True, msra_norm=True, msra_skip=True,\n",
    "\t\tqkv_bias=True, drop=0., attn_drop=0., drop_path=0.05,\n",
    "\t\top_names=[['conv'], ['conv'], ['mdmsa', 'conv'], ['mdmsa', 'conv']],\n",
    "\t\td_groups=[2, 2, 2, 2], c_groups=[-1, -1, -1, -1], gli_split=True, gli_weight=True,\n",
    "\t\tmlp_ratio=4., cls_head_nums=0, **kwargs)\n",
    "\treturn model\n",
    "\n",
    "\n",
    "def eatformer_mini(pretrained=False, **kwargs):\n",
    "\tmodel = EATFormer(\n",
    "\t\t# in_dim=3, num_classes=1000,\n",
    "\t\tdepths=[2, 3, 8, 2], embed_dims=[64, 128, 256, 320], dim_heads=[32, 32, 32, 32], window_sizes=[7, 7, 7, 7],\n",
    "\t\tkernel_sizes=[3, 3, 3, 3], down_mode='kernel', dilations=[[1], [1], [1, 2, 3], [1, 2]],\n",
    "\t\tnorms=['bn_2d', 'bn_2d', 'bn_2d'], msra_mode='sum', msra_weight=True, msra_norm=True, msra_skip=True,\n",
    "\t\tqkv_bias=True, drop=0., attn_drop=0., drop_path=0.07,\n",
    "\t\top_names=[['conv'], ['conv'], ['msa', 'conv'], ['msa', 'conv']],\n",
    "\t\td_groups=[2, 2, 2, 2], c_groups=[-1, -1, -1, -1], gli_split=True, gli_weight=True,\n",
    "\t\tmlp_ratio=4., cls_head_nums=0, **kwargs)\n",
    "\treturn model\n",
    "\n",
    "\n",
    "def eatformer_small(pretrained=False, **kwargs):\n",
    "\tmodel = EATFormer(\n",
    "\t\t# in_dim=3, num_classes=1000,\n",
    "\t\tdepths=[3, 4, 12, 3], embed_dims=[64, 128, 320, 448], dim_heads=[32, 32, 32, 32], window_sizes=[7, 7, 7, 7],\n",
    "\t\tkernel_sizes=[3, 3, 3, 3], down_mode='kernel', dilations=[[1], [1], [1, 2, 3], [1, 2]],\n",
    "\t\tnorms=['bn_2d', 'bn_2d', 'bn_2d'], msra_mode='sum', msra_weight=True, msra_norm=True, msra_skip=True,\n",
    "\t\tqkv_bias=True, drop=0., attn_drop=0., drop_path=0.10,\n",
    "\t\top_names=[['conv'], ['conv'], ['mdmsa', 'conv'], ['mdmsa', 'conv']],\n",
    "\t\td_groups=[2, 2, 2, 2], c_groups=[-1, -1, -1, -1], gli_split=True, gli_weight=True,\n",
    "\t\tmlp_ratio=4., cls_head_nums=0, **kwargs)\n",
    "\treturn model\n",
    "\n",
    "\n",
    "def eatformer_medium(pretrained=False, **kwargs):\n",
    "\tmodel = EATFormer(\n",
    "\t\t# in_dim=3, num_classes=1000,\n",
    "\t\tdepths=[4, 5, 14, 4], embed_dims=[64, 160, 384, 512], dim_heads=[32, 32, 32, 32], window_sizes=[7, 7, 7, 7],\n",
    "\t\tkernel_sizes=[3, 3, 3, 3], down_mode='kernel', dilations=[[1], [1], [1, 2, 3], [1, 2]],\n",
    "\t\tnorms=['bn_2d', 'bn_2d', 'bn_2d'], msra_mode='sum', msra_weight=True, msra_norm=True, msra_skip=True,\n",
    "\t\tqkv_bias=True, drop=0., attn_drop=0., drop_path=0.2,\n",
    "\t\top_names=[['conv'], ['conv'], ['msa', 'conv'], ['msa', 'conv']],\n",
    "\t\td_groups=[2, 2, 2, 2], c_groups=[-1, -1, -1, -1], gli_split=True, gli_weight=True,\n",
    "\t\tmlp_ratio=4., cls_head_nums=0, **kwargs)\n",
    "\treturn model\n",
    "\n",
    "\n",
    "def eatformer_base(pretrained=False, **kwargs):\n",
    "\tmodel = EATFormer(\n",
    "\t\t# in_dim=3, num_classes=1000,\n",
    "\t\tdepths=[5, 6, 18, 5], embed_dims=[64, 160, 384, 512], dim_heads=[32, 32, 32, 32], window_sizes=[7, 7, 7, 7],\n",
    "\t\tkernel_sizes=[3, 3, 3, 3], down_mode='kernel', dilations=[[1], [1], [1, 2, 3], [1, 2]],\n",
    "\t\tnorms=['bn_2d', 'bn_2d', 'bn_2d'], msra_mode='sum', msra_weight=True, msra_norm=True, msra_skip=True,\n",
    "\t\tqkv_bias=True, drop=0., attn_drop=0., drop_path=0.25,\n",
    "\t\top_names=[['conv'], ['conv'], ['msa', 'conv'], ['msa', 'conv']],\n",
    "\t\td_groups=[2, 2, 2, 2], c_groups=[-1, -1, -1, -1], gli_split=True, gli_weight=True,\n",
    "\t\tmlp_ratio=4., cls_head_nums=0, **kwargs)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e501f2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T07:33:15.869014Z",
     "iopub.status.busy": "2025-06-13T07:33:15.868739Z",
     "iopub.status.idle": "2025-06-13T07:33:15.875531Z",
     "shell.execute_reply": "2025-06-13T07:33:15.874620Z"
    },
    "papermill": {
     "duration": 0.01469,
     "end_time": "2025-06-13T07:33:15.877458",
     "exception": false,
     "start_time": "2025-06-13T07:33:15.862768",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "class DualModel(nn.Module):\n",
    "    def __init__(self, pretrained_eatformer=False, pretrained_resnet=True):\n",
    "        super(DualModel, self).__init__()\n",
    "        \n",
    "        # EATFormer initialization\n",
    "        self.eat_model = eatformer_base(pretrained=pretrained_eatformer)\n",
    "        in_features_eat = self.eat_model.head.in_features\n",
    "        self.eat_model.head = nn.Identity()  # Remove classification head\n",
    "        \n",
    "        # ResNet initialization\n",
    "        self.resnet_model = models.resnet50(pretrained=pretrained_resnet)\n",
    "        in_features_resnet = self.resnet_model.fc.in_features\n",
    "        self.resnet_model.fc = nn.Identity()  # Remove classification head\n",
    "        \n",
    "        # Combined classifier\n",
    "        combined_features = in_features_eat + in_features_resnet\n",
    "        self.head = nn.Linear(combined_features, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through EATFormer\n",
    "        eat_features = self.eat_model(x)\n",
    "        \n",
    "        # Forward pass through ResNet\n",
    "        resnet_features = self.resnet_model(x)\n",
    "        \n",
    "        # Concatenate features\n",
    "        combined_features = torch.cat((eat_features, resnet_features), dim=1)\n",
    "        \n",
    "        # Classification head\n",
    "        output = self.head(combined_features)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3523888d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T07:33:15.889678Z",
     "iopub.status.busy": "2025-06-13T07:33:15.889431Z",
     "iopub.status.idle": "2025-06-13T07:33:17.225750Z",
     "shell.execute_reply": "2025-06-13T07:33:17.225078Z"
    },
    "papermill": {
     "duration": 1.345252,
     "end_time": "2025-06-13T07:33:17.227651",
     "exception": false,
     "start_time": "2025-06-13T07:33:15.882399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, json\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Dataset: Video → fixed‑len Seq of probs\n",
    "# ---------------------------\n",
    "class CelebDFSeqDataset(Dataset):\n",
    "    def __init__(self, root_dir, split,\n",
    "                 base_model, transform,\n",
    "                 max_len=50):\n",
    "        \"\"\"\n",
    "        root_dir/\n",
    "          real/{video_id}/frame*.jpg\n",
    "          fake/{video_id}/frame*.jpg\n",
    "        split is unused here (Train vs Test folder passed separately).\n",
    "        \"\"\"\n",
    "        self.real_dir = os.path.join(root_dir, \"real\")\n",
    "        self.fake_dir = os.path.join(root_dir, \"fake\")\n",
    "\n",
    "        # build (frame_paths_list, label) for each video\n",
    "        self.videos = []\n",
    "        for label, d in [(1, self.real_dir), (0, self.fake_dir)]:\n",
    "            for vid in os.listdir(d):\n",
    "                vid_dir = os.path.join(d, vid)\n",
    "                if not os.path.isdir(vid_dir): continue\n",
    "                frames = sorted([\n",
    "                    os.path.join(vid_dir, f)\n",
    "                    for f in os.listdir(vid_dir)\n",
    "                    if f.lower().endswith(('.jpg','.png'))\n",
    "                ])\n",
    "                if not frames: continue\n",
    "                self.videos.append((frames, label))\n",
    "\n",
    "        self.base      = base_model.eval()\n",
    "        self.dev       = next(self.base.parameters()).device\n",
    "        self.transform = transform\n",
    "        self.max_len   = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.videos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frames, label = self.videos[idx]\n",
    "        L = len(frames)\n",
    "\n",
    "        # uniformly sample indices up to max_len\n",
    "        if L > self.max_len:\n",
    "            idxs = np.linspace(0, L-1, self.max_len, dtype=int)\n",
    "        else:\n",
    "            idxs = np.arange(L, dtype=int)\n",
    "\n",
    "        probs = []\n",
    "        with torch.no_grad():\n",
    "            for i in idxs:\n",
    "                img = Image.open(frames[i]).convert(\"RGB\")\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                img = img.unsqueeze(0).to(self.dev)\n",
    "                out = self.base(img)\n",
    "                probs.append(torch.sigmoid(out).item())\n",
    "\n",
    "        # pad if needed\n",
    "        if len(probs) < self.max_len:\n",
    "            probs += [0.0] * (self.max_len - len(probs))\n",
    "\n",
    "        # (max_len, 1)\n",
    "        seq = torch.tensor(probs, dtype=torch.float).unsqueeze(-1)\n",
    "        return seq, torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "# ---------------------------\n",
    "# 2) LSTM Classifier\n",
    "# ---------------------------\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_size=32, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "                            batch_first=True, bidirectional=True)\n",
    "        self.fc   = nn.Linear(hidden_size*2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)       # (B, seq, hid*2)\n",
    "        out     = out[:, -1]        # take last step\n",
    "        return self.fc(out).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2799ed2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T07:33:17.237822Z",
     "iopub.status.busy": "2025-06-13T07:33:17.237446Z",
     "iopub.status.idle": "2025-06-13T07:33:20.289886Z",
     "shell.execute_reply": "2025-06-13T07:33:20.289216Z"
    },
    "papermill": {
     "duration": 3.059675,
     "end_time": "2025-06-13T07:33:20.291857",
     "exception": false,
     "start_time": "2025-06-13T07:33:17.232182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:00<00:00, 158MB/s]\n",
      "/tmp/ipykernel_23/1802649416.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(\"/kaggle/working/eatformer_binary_classification.pth\",\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 3) Setup & load DualModel\n",
    "# ---------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "base_model = DualModel(pretrained_eatformer=False,\n",
    "                       pretrained_resnet=True).to(device)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    base_model = nn.DataParallel(base_model)\n",
    "\n",
    "# load your pre‑trained face/fake classifier\n",
    "ckpt = torch.load(\"/kaggle/working/eatformer_binary_classification.pth\",\n",
    "                  map_location=device)\n",
    "base_model.load_state_dict(ckpt)\n",
    "base_model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],\n",
    "                         [0.229,0.224,0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4355d5f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T07:33:20.303189Z",
     "iopub.status.busy": "2025-06-13T07:33:20.302897Z",
     "iopub.status.idle": "2025-06-13T14:41:53.214449Z",
     "shell.execute_reply": "2025-06-13T14:41:53.213593Z"
    },
    "papermill": {
     "duration": 25712.936303,
     "end_time": "2025-06-13T14:41:53.233542",
     "exception": false,
     "start_time": "2025-06-13T07:33:20.297239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extract train seqs:   0%|                               | 0/188 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py:79: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.device(device), torch.cuda.stream(stream), autocast(enabled=autocast_enabled):\n",
      "Extract train seqs: 100%|██████████████████| 188/188 [7:07:33<00:00, 136.46s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# ---------------------------\n",
    "# 4) Build train sequences\n",
    "# ---------------------------\n",
    "train_ds = CelebDFSeqDataset(\n",
    "    root_dir=\"/kaggle/input/celebdfv2/crop/Train\",\n",
    "    split=\"Train\",\n",
    "    base_model=base_model,\n",
    "    transform=transform,\n",
    "    max_len=40\n",
    ")\n",
    "train_loader = DataLoader(train_ds, batch_size=32,\n",
    "                          shuffle=True, num_workers=0)\n",
    "\n",
    "all_seqs, all_labels = [], []\n",
    "for seqs, labs in tqdm(train_loader, desc=\"Extract train seqs\", ncols=80):\n",
    "    all_seqs.append(seqs)\n",
    "    all_labels.append(labs)\n",
    "all_seqs   = torch.cat(all_seqs, dim=0)   # (N,50,1)\n",
    "all_labels = torch.cat(all_labels, dim=0) # (N,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0a6421e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T14:41:53.260529Z",
     "iopub.status.busy": "2025-06-13T14:41:53.260222Z",
     "iopub.status.idle": "2025-06-13T14:41:58.933756Z",
     "shell.execute_reply": "2025-06-13T14:41:58.932677Z"
    },
    "papermill": {
     "duration": 5.689121,
     "end_time": "2025-06-13T14:41:58.935603",
     "exception": false,
     "start_time": "2025-06-13T14:41:53.246482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 — Loss: 0.2671\n",
      "Epoch 2/10 — Loss: 0.0180\n",
      "Epoch 3/10 — Loss: 0.0037\n",
      "Epoch 4/10 — Loss: 0.0018\n",
      "Epoch 5/10 — Loss: 0.0011\n",
      "Epoch 6/10 — Loss: 0.0008\n",
      "Epoch 7/10 — Loss: 0.0005\n",
      "Epoch 8/10 — Loss: 0.0004\n",
      "Epoch 9/10 — Loss: 0.0003\n",
      "Epoch 10/10 — Loss: 0.0003\n",
      "✅ Saved LSTM → celeBdf_lstm.pth\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 5) Train LSTM\n",
    "# ---------------------------\n",
    "lstm_model = LSTMClassifier().to(device)\n",
    "optimizer  = optim.Adam(lstm_model.parameters(), lr=1e-3)\n",
    "criterion  = nn.BCEWithLogitsLoss()\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(all_seqs, all_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32,\n",
    "                          shuffle=True, num_workers=0)\n",
    "\n",
    "best_auc = 0\n",
    "for epoch in range(1, 11):\n",
    "    lstm_model.train()\n",
    "    total_loss = 0\n",
    "    for seqs, labs in train_loader:\n",
    "        seqs, labs = seqs.to(device), labs.to(device)\n",
    "        logits     = lstm_model(seqs)\n",
    "        loss       = criterion(logits, labs)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * seqs.size(0)\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch}/10 — Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# save LSTM\n",
    "torch.save({\"model_state_dict\": lstm_model.state_dict()},\n",
    "           \"celebdf_lstm.pth\")\n",
    "print(\"✅ Saved LSTM → celeBdf_lstm.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "978d4e82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T14:41:58.963562Z",
     "iopub.status.busy": "2025-06-13T14:41:58.963061Z",
     "iopub.status.idle": "2025-06-13T15:18:29.179497Z",
     "shell.execute_reply": "2025-06-13T15:18:29.178719Z"
    },
    "papermill": {
     "duration": 2190.23204,
     "end_time": "2025-06-13T15:18:29.181092",
     "exception": false,
     "start_time": "2025-06-13T14:41:58.949052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing LSTM: 100%|██████████| 33/33 [36:24<00:00, 66.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test → AUC: 0.9993, F1: 0.9888, Acc: 0.9923\n",
      "✅ Saved ROC data to roc_data_celebdf_lstm.json\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 6) Test & save ROC/metrics\n",
    "# ---------------------------\n",
    "test_ds = CelebDFSeqDataset(\n",
    "    root_dir=\"/kaggle/input/celebdfv2/crop/Test\",\n",
    "    split=\"Test\",\n",
    "    base_model=base_model,\n",
    "    transform=transform,\n",
    "    max_len=40\n",
    ")\n",
    "test_loader = DataLoader(test_ds, batch_size=16,\n",
    "                         shuffle=False, num_workers=0)\n",
    "\n",
    "lstm_model.eval()\n",
    "preds, labs = [], []\n",
    "with torch.no_grad():\n",
    "    for seqs, lb in tqdm(test_loader, desc=\"Testing LSTM\"):\n",
    "        seqs = seqs.to(device)\n",
    "        out  = lstm_model(seqs)\n",
    "        pr   = torch.sigmoid(out).cpu().tolist()\n",
    "        preds.extend(pr)\n",
    "        labs.extend(lb.tolist())\n",
    "\n",
    "# compute metrics\n",
    "auc  = roc_auc_score(labs, preds)\n",
    "binp = [1 if p>0.5 else 0 for p in preds]\n",
    "f1   = f1_score(labs, binp)\n",
    "acc  = accuracy_score(labs, binp)\n",
    "print(f\"Test → AUC: {auc:.4f}, F1: {f1:.4f}, Acc: {acc:.4f}\")\n",
    "\n",
    "# save metrics\n",
    "json.dump({\"auc\":auc, \"f1\":f1, \"acc\":acc},\n",
    "          open(\"metrics_celebdf_lstm.json\",\"w\"))\n",
    "\n",
    "# ROC curve data\n",
    "fpr, tpr, thr = roc_curve(labs, preds)\n",
    "roc_data = {\n",
    "    \"fpr\": fpr.tolist(),\n",
    "    \"tpr\": tpr.tolist(),\n",
    "    \"thresholds\": thr.tolist(),\n",
    "    \"auc\": auc\n",
    "}\n",
    "json.dump(roc_data, open(\"roc_data_celebdf_lstm.json\",\"w\"), indent=2)\n",
    "print(\"✅ Saved ROC data to roc_data_celebdf_lstm.json\")\n",
    "\n",
    "# optional: plot\n",
    "plt.plot(fpr, tpr, label=f\"LSTM AUC={auc:.3f}\")\n",
    "plt.plot([0,1],[0,1],\"--\", color=\"gray\")\n",
    "plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC – CelebDF LSTM\")\n",
    "plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb0a9d9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T15:18:29.213134Z",
     "iopub.status.busy": "2025-06-13T15:18:29.212489Z",
     "iopub.status.idle": "2025-06-13T15:18:37.104167Z",
     "shell.execute_reply": "2025-06-13T15:18:37.103215Z"
    },
    "papermill": {
     "duration": 7.909263,
     "end_time": "2025-06-13T15:18:37.105822",
     "exception": false,
     "start_time": "2025-06-13T15:18:29.196559",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Celeb‑DF Train/real → '00007' has the most frames: 92\n",
      "Celeb‑DF Train/fake → 'id55_id49_0002' has the most frames: 85\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def find_max_frames(root_dir, exts=('.jpg', '.png')):\n",
    "    \"\"\"\n",
    "    Walks each immediate subfolder of root_dir, counts files ending with exts,\n",
    "    and returns (subfolder_name, frame_count) for the folder with the maximum.\n",
    "    \"\"\"\n",
    "    max_count = 0\n",
    "    max_folder = None\n",
    "\n",
    "    for folder in sorted(os.listdir(root_dir)):\n",
    "        folder_path = os.path.join(root_dir, folder)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        # count only files with the given extensions\n",
    "        count = sum(\n",
    "            1\n",
    "            for f in os.listdir(folder_path)\n",
    "            if f.lower().endswith(exts)\n",
    "        )\n",
    "\n",
    "        if count > max_count:\n",
    "            max_count = count\n",
    "            max_folder = folder\n",
    "\n",
    "    return max_folder, max_count\n",
    "\n",
    "# Example usage for Celeb‑DF:\n",
    "celebd_root = \"/kaggle/input/celebdfv2/crop/Train\"\n",
    "for label in [\"real\", \"fake\"]:\n",
    "    path = os.path.join(celebd_root, label)\n",
    "    folder, cnt = find_max_frames(path)\n",
    "    print(f\"Celeb‑DF Train/{label} → {folder!r} has the most frames: {cnt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd405329",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-13T15:18:37.137355Z",
     "iopub.status.busy": "2025-06-13T15:18:37.137082Z",
     "iopub.status.idle": "2025-06-13T15:18:41.479389Z",
     "shell.execute_reply": "2025-06-13T15:18:41.478609Z"
    },
    "papermill": {
     "duration": 4.359663,
     "end_time": "2025-06-13T15:18:41.480916",
     "exception": false,
     "start_time": "2025-06-13T15:18:37.121253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CelebDF Train/real:  max=92 @ '00007',   avg=41.4\n",
      "CelebDF Train/fake:  max=85 @ 'id55_id49_0002',   avg=38.4\n",
      "CelebDF Test/real:  max=73 @ '00275',   avg=40.2\n",
      "CelebDF Test/fake:  max=85 @ 'id55_id57_0002',   avg=38.6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "def folder_frame_stats(root_dir, exts=('.jpg','.png')):\n",
    "    \"\"\"\n",
    "    For each immediate subfolder of root_dir, counts files ending with exts.\n",
    "    Returns (max_count, max_folder, avg_count).\n",
    "    \"\"\"\n",
    "    counts = []\n",
    "    for folder in sorted(os.listdir(root_dir)):\n",
    "        folder_path = os.path.join(root_dir, folder)\n",
    "        if not os.path.isdir(folder_path): \n",
    "            continue\n",
    "        c = sum(1 for f in os.listdir(folder_path)\n",
    "                if f.lower().endswith(exts))\n",
    "        counts.append((folder, c))\n",
    "    if not counts:\n",
    "        return None, 0, 0.0\n",
    "    # unpack\n",
    "    folders, cs = zip(*counts)\n",
    "    max_idx    = int(np.argmax(cs))\n",
    "    max_folder = folders[max_idx]\n",
    "    max_count  = cs[max_idx]\n",
    "    avg_count  = float(np.mean(cs))\n",
    "    return max_folder, max_count, avg_count\n",
    "\n",
    "# --- Celeb‑DF Train & Test ---\n",
    "for split in [\"Train\", \"Test\"]:\n",
    "    root = f\"/kaggle/input/celebdfv2/crop/{split}\"\n",
    "    for label in [\"real\", \"fake\"]:\n",
    "        path = os.path.join(root, label)\n",
    "        mf, mc, ac = folder_frame_stats(path)\n",
    "        print(f\"CelebDF {split}/{label}:  max={mc} @ '{mf}',   avg={ac:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c15bb6",
   "metadata": {
    "papermill": {
     "duration": 0.015041,
     "end_time": "2025-06-13T15:18:41.511868",
     "exception": false,
     "start_time": "2025-06-13T15:18:41.496827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5992461,
     "sourceId": 9781345,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6945747,
     "sourceId": 11136094,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6949497,
     "sourceId": 11140869,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 27966.594243,
   "end_time": "2025-06-13T15:18:43.883248",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-13T07:32:37.289005",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
